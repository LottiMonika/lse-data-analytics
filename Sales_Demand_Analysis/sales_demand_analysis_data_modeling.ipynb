{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovq7P5IAkbwi"
      },
      "source": [
        "# Library Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0798211a"
      },
      "outputs": [],
      "source": [
        "# --- Install required packages (run once in Colab) ---\n",
        "!pip install geopy\n",
        "!pip install chardet\n",
        "!pip install meteostat==1.6.8\n",
        "!pip install sktime\n",
        "!pip install -U scikit-learn\n",
        "!pip install nbformat\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- Core libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as stats\n",
        "\n",
        "# --- Visualization ---\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "import colorcet as cc\n",
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "sns.set_palette(cc.glasbey_cool)  # Set the palette for the notebook\n",
        "\n",
        "# --- Statsmodels ---\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "from statsmodels.graphics.gofplots import qqplot\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# --- Geopy for geocoding ---\n",
        "from geopy.exc import GeocoderTimedOut, GeocoderServiceError\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "# --- Meteostat for weather data ---\n",
        "import meteostat\n",
        "from meteostat import Daily\n",
        "\n",
        "# --- Scikit-learn ---\n",
        "from sklearn import metrics, tree, linear_model\n",
        "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    mean_absolute_error,\n",
        "    mean_absolute_percentage_error,\n",
        "    mean_squared_error,\n",
        "    r2_score,\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    root_mean_squared_error,\n",
        ")\n",
        "from sklearn.model_selection import (\n",
        "    GridSearchCV,\n",
        "    TimeSeriesSplit,\n",
        "    RandomizedSearchCV,\n",
        "    BaseCrossValidator,\n",
        "    cross_val_score,\n",
        "    train_test_split,\n",
        ")\n",
        "from sklearn.preprocessing import FunctionTransformer, LabelEncoder, StandardScaler, TargetEncoder\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
        "from sklearn.utils.validation import column_or_1d\n",
        "\n",
        "# --- Sktime (time series forecasting) ---\n",
        "from sktime.forecasting.model_selection import temporal_train_test_split\n",
        "from sktime.forecasting.compose import make_reduction\n",
        "from sktime.forecasting.base import ForecastingHorizon\n",
        "from sktime.forecasting.arima import AutoARIMA\n",
        "from sktime.forecasting.exp_smoothing import ExponentialSmoothing\n",
        "\n",
        "# --- Other standard libs and utilities ---\n",
        "import calendar\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "from typing import Tuple\n",
        "\n",
        "# --- Other third-party libs ---\n",
        "import nbformat\n",
        "import chardet\n",
        "import colorcet\n",
        "import matplotlib\n",
        "import numpy\n",
        "import pandas\n",
        "import scipy\n",
        "import seaborn\n",
        "import shap\n",
        "import statsmodels\n",
        "from scipy.stats import skew\n",
        "\n",
        "seed=42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2b0393f"
      },
      "source": [
        ">\n",
        "### __Define User Functions__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78d0574a"
      },
      "source": [
        "#### __Retrieve Lat and Lon from Postcodes__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d58bafb0"
      },
      "outputs": [],
      "source": [
        "def get_coordinates_from_postcode(postcode, max_retries=3):\n",
        "    \"\"\"\n",
        "    This function retrieves the geographic coordinates (latitude and longitude)\n",
        "    for a given Spanish postcode using the Nominatim geocoding service.\n",
        "\n",
        "    It attempts the request up to a specified number of retries in case of\n",
        "    timeouts or connection errors, with a delay between each retry to respect\n",
        "    the Nominatim usage policy.\n",
        "\n",
        "    Parameters:\n",
        "    - postcode (str): The postcode for which coordinates are to be retrieved.\n",
        "    - max_retries (int): Maximum number of retry attempts in case of failure\n",
        "      (default is 3).\n",
        "\n",
        "    Returns:\n",
        "    - A tuple containing (latitude, longitude) as floats.\n",
        "      Returns (None, None) if the location could not be retrieved.\n",
        "    \"\"\"\n",
        "    geolocator = Nominatim(user_agent=\"postcode_locator\", timeout=5) # Timeout after waiting 5 secs.\n",
        "    attempt = 0\n",
        "\n",
        "    while attempt < max_retries:\n",
        "        try:\n",
        "            print(f\"Getting coordinates for: {postcode}\")\n",
        "            location = geolocator.geocode(f\"{postcode}, country\", addressdetails=False)\n",
        "\n",
        "            if location:\n",
        "                time.sleep(1)  # Respect usage policy.\n",
        "                return location.latitude, location.longitude\n",
        "            else:\n",
        "                return None, None\n",
        "\n",
        "        except (GeocoderTimedOut, GeocoderServiceError) as e:\n",
        "            print(f\"Retry {attempt + 1} for postcode {postcode} due to error: {e}\")\n",
        "            attempt += 1\n",
        "            time.sleep(2) # Wait before retrying.\n",
        "\n",
        "    return None, None # If all retries fail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c1adf15"
      },
      "outputs": [],
      "source": [
        "def get_daily_temp_normals(\n",
        "    station_id='08221',\n",
        "    start_year=1991,\n",
        "    end_year=2020,\n",
        "    k_neighbors=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Retrieves daily reference normals (1991–2020 by default) for tmax, tmin, and tavg\n",
        "    using Meteostat data. Applies KNN imputation for missing values.\n",
        "\n",
        "    Parameters:\n",
        "    - station_id (str): Meteostat station ID (default: _____)\n",
        "    - start_year (int): Start year of the period (default: 1991)\n",
        "    - end_year (int): End year of the period (default: 2020)\n",
        "    - k_neighbors (int): Number of neighbors for KNN imputation (default: 5)\n",
        "\n",
        "    Returns:\n",
        "    - dict: Dictionary with keys 'ref_tmax', 'ref_tmin', 'ref_tavg', each mapping\n",
        "            day-of-year (1–365) to the corresponding normal temperature.\n",
        "    \"\"\"\n",
        "\n",
        "    # Fetch data from metropolitan area, the airport is considered to be\n",
        "    # a good representation of weather data for the Community.\n",
        "    # Set start and end dates for collection.\n",
        "    start = datetime(start_year, 1, 1)\n",
        "    end = datetime(end_year, 12, 31)\n",
        "\n",
        "    print(\"Fetching data...\")\n",
        "    data = Daily(station_id, start, end).fetch()\n",
        "    data = data[['tmax', 'tmin', 'tavg']].copy()\n",
        "\n",
        "    # Impute missing values using KNN.\n",
        "    print(f\"Missing values before imputation:\\n{data.isna().sum()}\")\n",
        "\n",
        "    if data.isna().any().any():\n",
        "        print(\"Imputing missing values using KNN...\")\n",
        "        imputer = KNNImputer(n_neighbors=k_neighbors)\n",
        "        data[['tmax', 'tmin', 'tavg']] = imputer.fit_transform(data[['tmax', 'tmin', 'tavg']])\n",
        "        print(\"Missing values imputed.\")\n",
        "    else:\n",
        "        print(\"No missing values found.\")\n",
        "\n",
        "    # Select data which is NOT February 29. This will leave gaps for leap years\n",
        "    # which will need to be imputed in the dataset.\n",
        "    data = data[~((data.index.month == 2) & (data.index.day == 29))]\n",
        "\n",
        "    # Add day-of-year column (now guaranteed to be 1–365).\n",
        "    data['day_of_year'] = data.index.dayofyear\n",
        "    data['day_of_year'] = data['day_of_year'].apply(lambda x: 365 if x == 366 else x)\n",
        "\n",
        "    # Group by day-of-year.\n",
        "    print(\"Calculating daily normals...\")\n",
        "    daily_normals = data.groupby('day_of_year').mean().reindex(range(1, 366))  # Ensure all 365 days.\n",
        "\n",
        "    # Format results to map.\n",
        "    result = {\n",
        "        'ref_tmax': daily_normals['tmax'].round(2).to_dict(),\n",
        "        'ref_tmin': daily_normals['tmin'].round(2).to_dict(),\n",
        "        'ref_tavg': daily_normals['tavg'].round(2).to_dict()\n",
        "    }\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ef40ef0"
      },
      "source": [
        "#### __One-hot Encode DataFrame Columns__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aa4f479"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode_columns(df, columns, prefix=True, drop_first=False, keep_original=False):\n",
        "    \"\"\"\n",
        "    One-hot encodes multiple columns that are of object type, with an option to keep the original columns.\n",
        "\n",
        "    Raises an error if any specified column is not of object or categorical type.\n",
        "\n",
        "    Parameters:\n",
        "    - df: the DataFrame which contains the relevant columns.\n",
        "    - columns: list of column names to one-hot encode.\n",
        "    - prefix: bool, if True adds the column name as a prefix to dummy columns;\n",
        "              default is True.\n",
        "    - drop_first: bool, if True drops the first category to avoid multicollinearity.\n",
        "    - keep_original: bool, if True retains the original columns; default is False.\n",
        "\n",
        "    Returns:\n",
        "    - df: DataFrame with one-hot encoded columns added, and optionally the original columns kept.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    for col in columns:\n",
        "        # Ensure the column is of object type.\n",
        "        if not (df[col].dtype == 'object' or pd.api.types.is_categorical_dtype(df[col])):\n",
        "            raise ValueError(f\"Column '{col}' is not of object type and cannot be one-hot encoded.\")\n",
        "\n",
        "        # One-hot encode the column.\n",
        "        dummies = pd.get_dummies(df[col], prefix=col if prefix else None, drop_first=drop_first)\n",
        "\n",
        "        # Concatenate the one-hot encoded columns to the DataFrame.\n",
        "        if keep_original:\n",
        "            df = pd.concat([df, dummies], axis=1)  # Keep the original column.\n",
        "        else:\n",
        "            df = pd.concat([df.drop(columns=[col]), dummies], axis=1)  # Drop the original column.\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a466615"
      },
      "source": [
        "#### __Add Cyclical Encoding for DataFrame Temporal Variables__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "854e05b7"
      },
      "outputs": [],
      "source": [
        "def add_cyclical_encoding(df, columns_periods, drop_original=True):\n",
        "    \"\"\"\n",
        "    Applies cyclical encoding (sine and cosine) to specified columns in a\n",
        "    DataFrame. Should be used on variables such as day of week which are\n",
        "    inherently cyclical. Variables should all be ordered\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame which contains the variables to be transformed.\n",
        "    - columns_periods: dict of column names and their respective periods.\n",
        "                       e.g., {'month': 12, 'day': 31}\n",
        "    - drop_original: bool, if True drops the original columns after encoding.\n",
        "\n",
        "    Returns:\n",
        "    - df: DataFrame with new cyclical features added.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    for col, period in columns_periods.items():\n",
        "        sin_col = f\"{col}_sin\"\n",
        "        cos_col = f\"{col}_cos\"\n",
        "\n",
        "        df[sin_col] = np.sin(2 * np.pi * df[col] / period)\n",
        "        df[cos_col] = np.cos(2 * np.pi * df[col] / period)\n",
        "\n",
        "        if drop_original:\n",
        "            df.drop(columns=[col], inplace=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlYFHlZMl9A1"
      },
      "source": [
        "#### __Determine Best TimeSeries Split__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZcabzK_klvX"
      },
      "outputs": [],
      "source": [
        "def evaluate_time_series_split_masked(model, df, y_col, split_range=range(3, 11), scoring='neg_mean_squared_error'):\n",
        "    \"\"\"\n",
        "    Evaluate a time series regression model using TimeSeriesSplit with varying numbers of splits.\n",
        "\n",
        "    Parameters:\n",
        "    - model: A scikit-learn compatible regressor instance (e.g., XGBRegressor(), LGBMRegressor()).\n",
        "    - df (pd.DataFrame): Full DataFrame containing features and target, indexed by datetime.\n",
        "    - y_col (str): Name of the target column.\n",
        "    - split_range (range): Range of split counts to evaluate (default is range(3, 11)).\n",
        "    - scoring (str): Scoring metric to use (default is 'neg_mean_squared_error').\n",
        "\n",
        "    Returns:\n",
        "    - mean_scores (list): List of mean scores for each split count.\n",
        "    - std_scores (list): List of standard deviations of scores for each split count.\n",
        "    \"\"\"\n",
        "\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
        "        raise ValueError(\"The DataFrame index must be of datetime type.\")\n",
        "\n",
        "    # Extract feature set (X) and target variable (y)\n",
        "    X = df.drop(columns=[y_col])\n",
        "    y = df[y_col]\n",
        "\n",
        "    # Get unique sorted dates for TimeSeriesSplit\n",
        "    unique_dates = df.index.unique().sort_values()\n",
        "\n",
        "    mean_scores = []\n",
        "    std_scores = []\n",
        "\n",
        "    for n_splits in split_range:\n",
        "        # Initialize TimeSeriesSplit with the current number of splits\n",
        "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "        fold_scores = []\n",
        "\n",
        "        # Perform cross-validation with time-based splits\n",
        "        for train_idx, test_idx in tscv.split(unique_dates):\n",
        "            train_dates = set(unique_dates[train_idx])\n",
        "            test_dates = set(unique_dates[test_idx])\n",
        "\n",
        "            # Mask based on whether index is in train or test dates\n",
        "            train_mask = df.index.map(train_dates.__contains__)\n",
        "            test_mask = df.index.map(test_dates.__contains__)\n",
        "\n",
        "            # Split the data\n",
        "            X_train, X_test = X[train_mask], X[test_mask]\n",
        "            y_train, y_test = y[train_mask], y[test_mask]\n",
        "\n",
        "            # Evaluate the model\n",
        "            model.fit(X_train, y_train)\n",
        "            score = cross_val_score(model, X_test, y_test, cv=tscv, scoring=scoring).mean()  # Evaluating only test set\n",
        "            fold_scores.append(score)\n",
        "\n",
        "        # Convert negative MSE to positive and calculate mean and std\n",
        "        mean_mse = -np.mean(fold_scores)  # We convert back to positive MSE\n",
        "        std_mse = np.std(fold_scores)\n",
        "\n",
        "        mean_scores.append(mean_mse)\n",
        "        std_scores.append(std_mse)\n",
        "\n",
        "        print(f\"{n_splits} splits: Mean MSE = {mean_mse:.4f}, Std = {std_mse:.4f}\")\n",
        "\n",
        "    # Plot the results\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(split_range, mean_scores, marker='o', label='Mean MSE', color='blue')\n",
        "    plt.fill_between(split_range,\n",
        "                     [m - s for m, s in zip(mean_scores, std_scores)],\n",
        "                     [m + s for m, s in zip(mean_scores, std_scores)],\n",
        "                     color='gray', alpha=0.2, label='±1 Std Dev')\n",
        "    plt.title('Evaluation of TimeSeriesSplit for Varying Split Counts', fontsize=14)\n",
        "    plt.xlabel('Number of Splits', fontsize=12)\n",
        "    plt.ylabel('Mean MSE', fontsize=12)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return mean_scores, std_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECaMVo5wmHfK"
      },
      "source": [
        "#### __Create TimeSeries Splits__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMax6NOaklrQ"
      },
      "outputs": [],
      "source": [
        "def get_timeseries_splits_with_masking(df, y, n_splits=5):\n",
        "    \"\"\"\n",
        "    Splits the data into training and testing sets using TimeSeriesSplit with datetime index.\n",
        "    Applies the same masking logic as used in the Random Forest model, ensuring no overlap\n",
        "    between the train and test sets. Prints the train/test indices for each fold.\n",
        "\n",
        "    Parameters:\n",
        "    - df (DataFrame): The DataFrame containing the feature variables and target variable.\n",
        "      The index must be of datetime type, and lag features should already be included.\n",
        "    - y (str): The target variable (dependent variable).\n",
        "    - n_splits (int, optional): The number of splits for TimeSeriesSplit. Default is 5.\n",
        "\n",
        "    Returns:\n",
        "    - list of tuples: Each tuple contains:\n",
        "      - X_train: Training feature data\n",
        "      - X_test: Testing feature data\n",
        "      - y_train: Training target data\n",
        "      - y_test: Testing target data\n",
        "    \"\"\"\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
        "        raise ValueError(\"The DataFrame index must be of datetime type.\")\n",
        "\n",
        "    feature_cols = [col for col in df.columns if col != y]  # Exclude the target variable.\n",
        "    X = df[feature_cols]\n",
        "    y_series = df[y]\n",
        "\n",
        "    # Get unique sorted dates for TimeSeriesSplit\n",
        "    unique_dates = df.index.unique().sort_values()\n",
        "\n",
        "    splits = []\n",
        "\n",
        "    # Initialize TimeSeriesSplit\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    # Generate train/test splits\n",
        "    for fold, (train_idx, test_idx) in enumerate(tscv.split(unique_dates)):\n",
        "        print(f\"Fold {fold + 1}\")\n",
        "\n",
        "        # Get train and test data by dates (unique sorted)\n",
        "        train_dates = set(unique_dates[train_idx])\n",
        "        test_dates = set(unique_dates[test_idx])\n",
        "\n",
        "        # Mask based on whether index is in train or test dates\n",
        "        train_mask = df.index.map(train_dates.__contains__)\n",
        "        test_mask = df.index.map(test_dates.__contains__)\n",
        "\n",
        "        # Create the final train and test sets based on these masks\n",
        "        X_train, X_test = X[train_mask], X[test_mask]\n",
        "        y_train, y_test = y_series[train_mask], y_series[test_mask]\n",
        "\n",
        "        # Check for overlap of dates between train and test\n",
        "        overlap_dates = np.intersect1d(df.index[train_mask], df.index[test_mask])\n",
        "\n",
        "        if len(overlap_dates) > 0:\n",
        "            print(f\"Warning: Duplicate dates found between train and test \"\n",
        "                  f\"for Fold {fold + 1} of {n_splits}: {overlap_dates}\")\n",
        "        else:\n",
        "            print(f\"No duplicates between train and test for Fold {fold + 1} of {n_splits}\")\n",
        "            print(\"Proceeding with train-test split...\")\n",
        "\n",
        "        # Store the splits\n",
        "        splits.append((X_train, X_test, y_train, y_test))\n",
        "\n",
        "        # Print out the train-test split details\n",
        "        print(f\"Train indices: {train_idx}\")\n",
        "        print(f\"Test indices: {test_idx}\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    return splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krSBUu6_mrYO"
      },
      "source": [
        "#### __Check for Overfitting on Last Fold of Boosted Regression Model__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dThbj5JwlBE0"
      },
      "outputs": [],
      "source": [
        "def check_overfitting_on_last_fold_masked(df, y_col, n_splits, model):\n",
        "    \"\"\"\n",
        "    Checks for overfitting by training on the last fold of TimeSeriesSplit and comparing\n",
        "    R² scores on train and test sets.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): Full DataFrame containing features and target, indexed by datetime.\n",
        "    - y_col (str): Name of the target column.\n",
        "    - n_splits (int): Number of TimeSeriesSplit folds.\n",
        "    - model: A model instance (e.g., best XGBoost model).\n",
        "\n",
        "    Returns:\n",
        "    - train_r2: R² on training set.\n",
        "    - test_r2: R² on test set.\n",
        "    \"\"\"\n",
        "\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
        "        raise ValueError(\"The DataFrame index must be of datetime type.\")\n",
        "\n",
        "    # Extract feature set (X) and target variable (y)\n",
        "    X = df.drop(columns=[y_col])\n",
        "    y = df[y_col]\n",
        "\n",
        "    # Get unique sorted dates\n",
        "    unique_dates = df.index.unique().sort_values()\n",
        "\n",
        "    # Initialize TimeSeriesSplit\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    # Track the last fold's train and test indices\n",
        "    for fold, (train_idx, test_idx) in enumerate(tscv.split(unique_dates)):\n",
        "        pass  # Loop until the last fold, don't train yet\n",
        "\n",
        "    # Get the last fold's train and test date sets\n",
        "    train_dates = set(unique_dates[train_idx])\n",
        "    test_dates = set(unique_dates[test_idx])\n",
        "\n",
        "    # Mask based on whether index is in train or test dates\n",
        "    train_mask = df.index.map(train_dates.__contains__)\n",
        "    test_mask = df.index.map(test_dates.__contains__)\n",
        "\n",
        "    # Create train and test datasets based on the mask\n",
        "    X_train, X_test = X[train_mask], X[test_mask]\n",
        "    y_train, y_test = y[train_mask], y[test_mask]\n",
        "\n",
        "    # Fit the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "    train_pred = model.predict(X_train)\n",
        "    test_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate R² for both train and test sets\n",
        "\n",
        "    train_r2 = r2_score(y_train, train_pred)\n",
        "    test_r2 = r2_score(y_test, test_pred)\n",
        "\n",
        "    # Output the R² scores and check for overfitting\n",
        "    print(f\"\\nOverfitting Check (Last Fold):\")\n",
        "    print(f\"Train R²: {train_r2:.4f}\")\n",
        "    print(f\"Test  R²: {test_r2:.4f}\")\n",
        "\n",
        "    if train_r2 - test_r2 > 0.2:\n",
        "        print(\"⚠️ Model likely overfitting. Consider simplifying or regularizing.\")\n",
        "    else:\n",
        "        print(\"✅ No significant overfitting detected.\")\n",
        "\n",
        "    return train_r2, test_r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzN0mcaxvV3F"
      },
      "source": [
        "#### __Generate Static Calendar Feature__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE4xTo7LvvRQ"
      },
      "outputs": [],
      "source": [
        "def generate_static_calendar_features(year: int) -> pd.DataFrame:\n",
        "    # Generate daily date range for the given year\n",
        "    future_dates = pd.date_range(f\"{year}-01-01\", f\"{year}-12-31\", freq='D')\n",
        "\n",
        "    df = pd.DataFrame({\"date\": future_dates})\n",
        "\n",
        "    # Extract basic calendar components\n",
        "    df[\"year\"] = df[\"date\"].dt.year\n",
        "    df[\"month\"] = df[\"date\"].dt.month.astype(np.int32)\n",
        "    df[\"day\"] = df[\"date\"].dt.day\n",
        "    df[\"day_of_week\"] = df[\"date\"].dt.dayofweek  # Monday=0\n",
        "    df[\"day_of_year\"] = df[\"date\"].dt.dayofyear\n",
        "\n",
        "    # Add cyclical features for month and weekday\n",
        "    df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
        "    df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
        "    df[\"day_of_week_sin\"] = np.sin(2 * np.pi * df[\"day_of_week\"] / 7)\n",
        "    df[\"day_of_week_cos\"] = np.cos(2 * np.pi * df[\"day_of_week\"] / 7)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0qhoOkPLVNK"
      },
      "source": [
        "#### __Create Forecast Panel__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qdbcv5m5yOG-"
      },
      "outputs": [],
      "source": [
        "def create_forecast_panel(X, df_unpaneled, station_code='sales_location',\n",
        "                               one_hot_columns=['province', 'town/city', 'company_code']):\n",
        "    \"\"\"\n",
        "    Create forecast panel with one row per date per sales station by joining\n",
        "    df_unpaneled with one-hot encoded station columns extracted exactly from X.\n",
        "\n",
        "    Parameters:\n",
        "    - X: pd.DataFrame\n",
        "        Training DataFrame containing one-hot encoded columns and station_code.\n",
        "    - df_unpaneled: pd.DataFrame\n",
        "        DataFrame with daily weather stats indexed by date.\n",
        "    - station_code: str, default 'sales_location'\n",
        "        Column name identifying sales locations in X.\n",
        "    - one_hot_columns: list of str\n",
        "        Exact one-hot encoded column names to extract from X.\n",
        "\n",
        "    Returns:\n",
        "    - forecast_panel: pd.DataFrame\n",
        "        One row per date per station with all features combined.\n",
        "    \"\"\"\n",
        "    # Extract station info columns exactly as encoded in X\n",
        "    station_lookup = X[[station_code] + one_hot_columns].drop_duplicates(subset=[station_code]).reset_index(drop=True)\n",
        "\n",
        "    # Add dummy key to enable cross join\n",
        "    df_unpaneled = df_unpaneled.copy()\n",
        "    station_lookup = station_lookup.copy()\n",
        "    df_unpaneled['key'] = 1\n",
        "    station_lookup['key'] = 1\n",
        "\n",
        "    # Cross join daily scenarios with station lookup\n",
        "    df_paneled = pd.merge(df_unpaneled, station_lookup, on='key').drop('key', axis=1)\n",
        "\n",
        "    return df_paneled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "839dA-OgyNz_"
      },
      "source": [
        "#### __Create Historical Climate Data__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cbf6b33"
      },
      "outputs": [],
      "source": [
        "def get_daily_tmax_normals_city(station_id='08221', k_neighbors=5):\n",
        "    \"\"\"\n",
        "    This function retrieves the daily values for the city for the relevant 30 year\n",
        "    Period 1991-2020 and calculates daily reference normals using the Meteostat\n",
        "    package. It checks for missing climate values and imputes any missing values\n",
        "    with KNN imputation, returning a dictionary mapping day-of-year (1–365) to\n",
        "    normal tmax.\n",
        "\n",
        "    Parameters:\n",
        "    - station_id (optional, str): The identifier for the weather station of choice;\n",
        "                                  defaults to the city.\n",
        "    - k_neighbors (optional, int): The k value for KNN imputation; defaults to 5.\n",
        "\n",
        "    Returns:\n",
        "    - Daily reference normals for 1991-2020.\n",
        "    \"\"\"\n",
        "    # Fetch data from the city, the airport is considered to be\n",
        "    # a good representation of weather data for the Community.\n",
        "    # Set start and end dates for collection.\n",
        "    start = datetime(1991, 1, 1)\n",
        "    end = datetime(2020, 12, 31)\n",
        "\n",
        "    print(\"Fetching data...\")\n",
        "    data = Daily(station_id, start, end).fetch()\n",
        "    data = data[['tmax']].copy()\n",
        "\n",
        "    # Impute missing values using KNN.\n",
        "    missing_count = data['tmax'].isna().sum()\n",
        "    print(f\"Missing days before imputation: {missing_count}\")\n",
        "\n",
        "    if data['tmax'].isna().sum() > 0:\n",
        "        print(\"Imputing missing tmax values before further processing...\")\n",
        "        imputer = KNNImputer(n_neighbors=k_neighbors)\n",
        "        data[['tmax']] = imputer.fit_transform(data[['tmax']])\n",
        "        print(\"Missing values imputed using KNN.\")\n",
        "    else:\n",
        "        print(\"No missing tmax values found.\")\n",
        "\n",
        "    # Select data which is NOT February 29. This will leave gaps for leap years\n",
        "    # which will need to be imputed in the dataset.\n",
        "    data = data[~((data.index.month == 2) & (data.index.day == 29))]\n",
        "\n",
        "    # Add day-of-year column (now guaranteed to be 1–365)\n",
        "    data['day_of_year'] = data.index.dayofyear\n",
        "    data['day_of_year'] = data['day_of_year'].replace(366, 365)\n",
        "\n",
        "    # Group by day-of-year.\n",
        "    print(\"Calculating daily normals...\")\n",
        "    daily_normals = data.groupby('day_of_year').mean()\n",
        "    daily_normals = daily_normals.reindex(range(1, 366))  # Ensure all 365 days.\n",
        "\n",
        "    # Final formatting\n",
        "    daily_normals = daily_normals.rename(columns={'tmax': 'ref_tmax'})\n",
        "    daily_normals.index.name = 'day_of_year'\n",
        "\n",
        "    return daily_normals['ref_tmax'].round(2).to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZjodpSHq73T"
      },
      "outputs": [],
      "source": [
        "def get_daily_temp_normals(\n",
        "    station_id='08221',\n",
        "    start_year=1991,\n",
        "    end_year=2020,\n",
        "    k_neighbors=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Retrieves daily reference normals (1991–2020 by default) for tmax, tmin, and tavg\n",
        "    using Meteostat data. Applies KNN imputation for missing values.\n",
        "\n",
        "    Parameters:\n",
        "    - station_id (str): Meteostat station ID (default: ____)\n",
        "    - start_year (int): Start year of the period (default: 1991)\n",
        "    - end_year (int): End year of the period (default: 2020)\n",
        "    - k_neighbors (int): Number of neighbors for KNN imputation (default: 5)\n",
        "\n",
        "    Returns:\n",
        "    - dict: Dictionary with keys 'ref_tmax', 'ref_tmin', 'ref_tavg', each mapping\n",
        "            day-of-year (1–365) to the corresponding normal temperature.\n",
        "    \"\"\"\n",
        "\n",
        "    # Fetch data from the city, the airport is considered to be\n",
        "    # a good representation of weather data for the Community.\n",
        "    # Set start and end dates for collection.\n",
        "    start = datetime(start_year, 1, 1)\n",
        "    end = datetime(end_year, 12, 31)\n",
        "\n",
        "    print(\"Fetching data...\")\n",
        "    data = Daily(station_id, start, end).fetch()\n",
        "    data = data[['tmax', 'tmin', 'tavg']].copy()\n",
        "\n",
        "    # Impute missing values using KNN.\n",
        "    print(f\"Missing values before imputation:\\n{data.isna().sum()}\")\n",
        "\n",
        "    if data.isna().any().any():\n",
        "        print(\"Imputing missing values using KNN...\")\n",
        "        imputer = KNNImputer(n_neighbors=k_neighbors)\n",
        "        data[['tmax', 'tmin', 'tavg']] = imputer.fit_transform(data[['tmax', 'tmin', 'tavg']])\n",
        "        print(\"Missing values imputed.\")\n",
        "    else:\n",
        "        print(\"No missing values found.\")\n",
        "\n",
        "    # Select data which is NOT February 29. This will leave gaps for leap years\n",
        "    # which will need to be imputed in the dataset.\n",
        "    data = data[~((data.index.month == 2) & (data.index.day == 29))]\n",
        "\n",
        "    # Add day-of-year column (now guaranteed to be 1–365).\n",
        "    data['day_of_year'] = data.index.dayofyear\n",
        "    data['day_of_year'] = data['day_of_year'].apply(lambda x: 365 if x == 366 else x)\n",
        "\n",
        "    # Group by day-of-year.\n",
        "    print(\"Calculating daily normals...\")\n",
        "    daily_normals = data.groupby('day_of_year').mean().reindex(range(1, 366))  # Ensure all 365 days.\n",
        "\n",
        "    # Format results to map.\n",
        "    result = {\n",
        "        'ref_tmax': daily_normals['tmax'].round(2).to_dict(),\n",
        "        'ref_tmin': daily_normals['tmin'].round(2).to_dict(),\n",
        "        'ref_tavg': daily_normals['tavg'].round(2).to_dict()\n",
        "    }\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMAk4LnyiwgQ"
      },
      "outputs": [],
      "source": [
        "def get_historical_daily_temps(station_id='08221', start_year=1991, end_year=2022, k_neighbors=5):\n",
        "    \"\"\"\n",
        "    Fetches and cleans historical daily temperature data (tmax, tmin, tavg) for a given station.\n",
        "\n",
        "    - Removes February 29 (leap day)\n",
        "    - Applies KNN imputation to fill missing temperature values\n",
        "    - Adds `date`, `day_of_year`, and `year` columns\n",
        "\n",
        "    Parameters:\n",
        "        station_id (str): Meteostat station ID (default: ____)\n",
        "        start_year (int): Start year for fetching historical data\n",
        "        end_year (int): End year for fetching historical data\n",
        "        k_neighbors (int): Number of neighbors for KNN imputation\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Cleaned historical weather data with columns: ['tmax', 'tmin', 'tavg', 'date', 'day_of_year', 'year']\n",
        "    \"\"\"\n",
        "\n",
        "    start = datetime(start_year, 1, 1)\n",
        "    end = datetime(end_year, 12, 31)\n",
        "\n",
        "    print(\"Fetching historical data...\")\n",
        "    data = Daily(station_id, start, end).fetch()[['tmax', 'tmin', 'tavg']].copy()\n",
        "\n",
        "    # Check how many February 29th dates there are before removal\n",
        "    print(\"February 29th count before removal:\", len(data[(data.index.month == 2) & (data.index.day == 29)]))\n",
        "\n",
        "    # Remove leap day (Feb 29).\n",
        "    data = data[~((data.index.month == 2) & (data.index.day == 29))]\n",
        "\n",
        "    # Check how many February 29th dates remain after removal\n",
        "    print(\"February 29th count after removal:\", len(data[(data.index.month == 2) & (data.index.day == 29)]))\n",
        "\n",
        "    # Impute missing values\n",
        "    if data.isna().any().any():\n",
        "        print(\"Imputing missing values using KNN...\")\n",
        "        imputer = KNNImputer(n_neighbors=k_neighbors)\n",
        "        data[['tmax', 'tmin', 'tavg']] = imputer.fit_transform(data[['tmax', 'tmin', 'tavg']])\n",
        "        print(\"Missing values imputed.\")\n",
        "\n",
        "    data = data.round(2)\n",
        "    data['date'] = data.index\n",
        "    data['day_of_year'] = data.index.dayofyear\n",
        "    data['day_of_year'] = data['day_of_year'].replace(366, 365)\n",
        "\n",
        "    # Add year column\n",
        "    data['year'] = data.index.year\n",
        "    data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4PxUwB5rYG3"
      },
      "outputs": [],
      "source": [
        "def get_daily_tmax_normals_mean_std(df, station_id='08221', k_neighbors=5):\n",
        "    \"\"\"\n",
        "    Retrieve daily maximum temperature (tmax) reference normals and calculate\n",
        "    historical deviation statistics for the city using Meteostat data (1991–2020).\n",
        "\n",
        "    This function:\n",
        "    - Downloads daily historical tmax data for the station.\n",
        "    - Removes leap day (Feb 29) to standardize to a 365-day year.\n",
        "    - Imputes missing values using KNN imputation.\n",
        "    - Computes daily reference normals by averaging tmax across all years per day-of-year.\n",
        "    - Calculates mean and standard deviation of daily deviations (actual - normal).\n",
        "\n",
        "    Parameters:\n",
        "    - station_id (str): Meteostat station ID (default: _____)\n",
        "    - k_neighbors (int): Number of neighbors for KNN imputation (default: 5)\n",
        "\n",
        "    Returns:\n",
        "    - df (pd.DataFrame): DataFrame with a DatetimeIndex to determine the time window.\n",
        "    - ref_tmax_dict: dict mapping day_of_year (1–365) → average tmax\n",
        "    - mean_deviation: float, mean of all daily temperature deviations\n",
        "    - std_deviation: float, std dev of all daily temperature deviations\n",
        "    \"\"\"\n",
        "    # Fetch data from the city. The airport is considered to be\n",
        "    # a good representation of weather data for the Community.\n",
        "\n",
        "    # Check DataFrame has datetime index.\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        raise ValueError(\"Input df must have a DatetimeIndex.\")\n",
        "\n",
        "    # Derive dynamic time window for data collection.\n",
        "    end_date = df.index.min()\n",
        "    start_date = end_date - pd.DateOffset(years=30)\n",
        "    print(f\"Fetching normals from {start_date.date()} to {end_date.date()}\")\n",
        "\n",
        "    # Fetch data from Meteostat.\n",
        "    print(\"Fetching data...\")\n",
        "    data = Daily(station_id, start_date, end_date).fetch()\n",
        "    data = data[['tmax']].copy()\n",
        "\n",
        "    # Impute missing values using KNN.\n",
        "    if data['tmax'].isna().sum() > 0:\n",
        "        print(f\"Missing values before imputation: {data['tmax'].isna().sum()}\")\n",
        "        imputer = KNNImputer(n_neighbors=k_neighbors)\n",
        "        data[['tmax']] = imputer.fit_transform(data[['tmax']])\n",
        "        print(\"Missing values imputed.\")\n",
        "\n",
        "    # Select data which is NOT February 29. This will leave gaps for leap years\n",
        "    # which will need to be imputed in the dataset.\n",
        "    data = data[~((data.index.month == 2) & (data.index.day == 29))]\n",
        "\n",
        "    # Add day of year.\n",
        "    data['day_of_year'] = data.index.dayofyear\n",
        "\n",
        "    # Calculate daily normals\n",
        "    print(\"Calculating daily normals...\")\n",
        "    daily_normals = data.groupby('day_of_year')['tmax'].mean().reindex(range(1, 366))\n",
        "    daily_normals.name = 'ref_tmax'\n",
        "\n",
        "    # Merge normals back for deviation calculation.\n",
        "    data = data.merge(daily_normals, left_on='day_of_year', right_index=True)\n",
        "    data['temp_deviation'] = data['tmax'] - data['ref_tmax']\n",
        "\n",
        "    # Calculate statistics.\n",
        "    mean_deviation = data['temp_deviation'].mean()\n",
        "    std_deviation = data['temp_deviation'].std()\n",
        "\n",
        "    print(f\"Mean deviation: {mean_deviation:.2f}, Std deviation: {std_deviation:.2f}\")\n",
        "\n",
        "    return daily_normals.round(2).to_dict(), mean_deviation, std_deviation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUzJrd2ypJ9U"
      },
      "outputs": [],
      "source": [
        "# Circular mean for wind direction\n",
        "def circular_mean_deg(series):\n",
        "    radians = np.deg2rad(series.dropna())\n",
        "    sin_sum = np.sum(np.sin(radians))\n",
        "    cos_sum = np.sum(np.cos(radians))\n",
        "    mean_angle = np.arctan2(sin_sum, cos_sum)\n",
        "    return np.rad2deg(mean_angle) % 360\n",
        "\n",
        "def mod_func(x):\n",
        "    return x.mode().iloc[0] if not x.mode().empty else np.nan\n",
        "\n",
        "def prcp_stats(x):\n",
        "    rain_days = x[x > 0]\n",
        "    prob = len(rain_days) / len(x)\n",
        "    if prob > 0:  # If rain has a probability greater than zero calculate stats otherwise rain stats should be 0.0 (prevents 0 days skewing aggregate calculations)\n",
        "        return pd.Series({\n",
        "            'prcp_rain_prob': prob,\n",
        "            'prcp_mean_if_rain': rain_days.mean(),\n",
        "            'prcp_max_if_rain': rain_days.max(),\n",
        "            'prcp_mode_if_rain': mod_func(rain_days)\n",
        "        })\n",
        "    else:\n",
        "        return pd.Series({\n",
        "            'prcp_rain_prob': 0.0,\n",
        "            'prcp_mean_if_rain': 0.0,\n",
        "            'prcp_max_if_rain': 0.0,\n",
        "            'prcp_mode_if_rain': 0.0\n",
        "        })\n",
        "\n",
        "def get_daily_variability(\n",
        "    station_id='08221',\n",
        "    start_year=1991,\n",
        "    end_year=2022,\n",
        "    k_neighbors=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Retrieves daily reference normals (1991–2020 by default) for tmax, tmin, and tavg\n",
        "    using Meteostat data. Applies KNN imputation for missing values.\n",
        "\n",
        "    Parameters:\n",
        "    - station_id (str): Meteostat station ID (default: ___)\n",
        "    - start_year (int): Start year of the period (default: 1991)\n",
        "    - end_year (int): End year of the period (default: 2020)\n",
        "    - k_neighbors (int): Number of neighbors for KNN imputation (default: 5)\n",
        "\n",
        "    Returns:\n",
        "    - dict: Dictionary with keys 'ref_tmax', 'ref_tmin', 'ref_tavg', each mapping\n",
        "            day-of-year (1–365) to the corresponding normal temperature.\n",
        "    \"\"\"\n",
        "\n",
        "    # Fetch data from the city the airport is considered to be\n",
        "    # a good representation of weather data for the Community.\n",
        "    # Set start and end dates for collection.\n",
        "    start = datetime(start_year, 1, 1)\n",
        "    end = datetime(end_year, 12, 31)\n",
        "\n",
        "    print(\"Fetching data...\")\n",
        "    data = Daily(station_id, start, end).fetch()\n",
        "    data = data[['tmax', 'tmin', 'tavg', 'pres', 'prcp', 'wspd', 'wdir']].copy()\n",
        "\n",
        "    # Separate imputation to preserve prcp = 0.0\n",
        "    to_impute = ['tmax', 'tmin', 'tavg', 'pres', 'wspd', 'wdir']\n",
        "    if data[to_impute + ['prcp']].isna().any().any():\n",
        "        print(\"Imputing missing values using KNN...\")\n",
        "\n",
        "        # Impute temperature and pressure only\n",
        "        imputer = KNNImputer(n_neighbors=k_neighbors)\n",
        "        data[to_impute] = imputer.fit_transform(data[to_impute])\n",
        "\n",
        "        # Handle precipitation imputation separately so that 0.0 values are retained.\n",
        "        prcp_missing = data['prcp'].isna()\n",
        "        if prcp_missing.any():\n",
        "            prcp_imputer = KNNImputer(n_neighbors=k_neighbors)\n",
        "            # Only fit on rows where prcp is not NaN\n",
        "            prcp_features = data[to_impute + ['prcp']]\n",
        "            imputed = prcp_imputer.fit_transform(prcp_features)\n",
        "            data['prcp'] = imputed[:, -1]  # last column is prcp\n",
        "\n",
        "        print(\"Missing values imputed.\")\n",
        "    else:\n",
        "        print(\"No missing values found.\")\n",
        "\n",
        "    # Check for missing values after imputation\n",
        "    print(f\"Missing values after imputation:\\n{data.isna().sum()}\")\n",
        "\n",
        "    # Check how many February 29th dates there are before removal\n",
        "    print(\"February 29th count before removal:\", len(data[(data.index.month == 2) & (data.index.day == 29)]))\n",
        "\n",
        "    # Remove leap day (Feb 29).\n",
        "    data = data[~((data.index.month == 2) & (data.index.day == 29))]\n",
        "\n",
        "    # Check how many February 29th dates remain after removal\n",
        "    print(\"February 29th count after removal:\", len(data[(data.index.month == 2) & (data.index.day == 29)]))\n",
        "\n",
        "    # Impute missing values using KNN.\n",
        "    print(f\"Missing values before imputation:\\n{data.isna().sum()}\")\n",
        "\n",
        "    # Add day-of-year column (now guaranteed to be 1–365).\n",
        "    data['day_of_year'] = data.index.dayofyear\n",
        "    data['day_of_year'] = data['day_of_year'].replace(366, 365)\n",
        "\n",
        "    # Group by day-of-year to calculate daily min, max, mean, and mode.\n",
        "    print(\"Calculating temperature and pressure normals...\")\n",
        "    core_stats = data.groupby('day_of_year').agg({\n",
        "        'tmax': ['min', 'mean', 'max', mod_func],\n",
        "        'tmin': ['min', 'mean', 'max', mod_func],\n",
        "        'tavg': ['min', 'mean', 'max', mod_func],\n",
        "        'pres': ['min', 'mean', 'max', mod_func],\n",
        "        'wspd': ['min', 'mean', 'max', mod_func],\n",
        "        'wdir': [circular_mean_deg]\n",
        "    })\n",
        "\n",
        "    # Aggregate precipitation separately\n",
        "    print(\"Calculating rain probability and rain-only stats...\")\n",
        "    rain_stats = data.groupby('day_of_year')['prcp'].apply(prcp_stats).unstack()\n",
        "\n",
        "    # Merge all daily climate stats\n",
        "    print(\"Merging all daily stats...\")\n",
        "    final_data = pd.concat([core_stats, rain_stats], axis=1)\n",
        "\n",
        "    print(\"Final data columns:\", final_data.columns)\n",
        "    print(\"Number of columns:\", len(final_data.columns))\n",
        "\n",
        "    # Flatten MultiIndex columns\n",
        "    final_data.columns = [\n",
        "        'tmax_min', 'tmax_mean', 'tmax_max', 'tmax_mode',\n",
        "        'tmin_min', 'tmin_mean', 'tmin_max', 'tmin_mode',\n",
        "        'tavg_min', 'tavg_mean', 'tavg_max', 'tavg_mode',\n",
        "        'avg_pressure_min', 'avg_pressure_mean', 'avg_pressure_max', 'avg_pressure_mode',\n",
        "        'precip_rain_prob', 'precip_mean_if_rain', 'precip_max_if_rain', 'precip_mode_if_rain',\n",
        "        'wind_speed_min', 'wind_speed_mean', 'wind_speed_max', 'wind_speed_mode',\n",
        "        'wind_dir_mean'\n",
        "    ]\n",
        "    return final_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePwJi_uIl2Va"
      },
      "source": [
        "#### __Create Climate Scenarios__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0Q1bCFPlA-1"
      },
      "outputs": [],
      "source": [
        "def generate_future_scenarios(temp_stats_hist, year=2023):\n",
        "    # Generate full-year date range\n",
        "    future_dates = pd.date_range(f\"{year}-01-01\", f\"{year}-12-31\", freq='D')\n",
        "\n",
        "    # Build scenario rows using historical stats\n",
        "    scenario_records = []\n",
        "    for date in future_dates:\n",
        "        day = date.timetuple().tm_yday\n",
        "        row = temp_stats_hist.loc[day]\n",
        "\n",
        "        scenario_records.append({\n",
        "            'date': date,\n",
        "            'day_of_year': day,\n",
        "\n",
        "            # Temperature\n",
        "            'tmax_min': row['tmax_min'],\n",
        "            'tmax_mean': row['tmax_mean'],\n",
        "            'tmax_max': row['tmax_max'],\n",
        "            'tmax_mode': row['tmax_mode'],\n",
        "\n",
        "            'tmin_min': row['tmin_min'],\n",
        "            'tmin_mean': row['tmin_mean'],\n",
        "            'tmin_max': row['tmin_max'],\n",
        "            'tmin_mode': row['tmin_mode'],\n",
        "\n",
        "            'tavg_min': row['tavg_min'],\n",
        "            'tavg_mean': row['tavg_mean'],\n",
        "            'tavg_max': row['tavg_max'],\n",
        "            'tavg_mode': row['tavg_mode'],\n",
        "\n",
        "            # Pressure\n",
        "            'avg_pressure_min': row['avg_pressure_min'],\n",
        "            'avg_pressure_mean': row['avg_pressure_mean'],\n",
        "            'avg_pressure_max': row['avg_pressure_max'],\n",
        "            'avg_pressure_mode': row['avg_pressure_mode'],\n",
        "\n",
        "            # Precipitation\n",
        "            'precip_rain_prob': row['precip_rain_prob'],\n",
        "            'precip_mean_if_rain': row['precip_mean_if_rain'],\n",
        "            'precip_max_if_rain': row['precip_max_if_rain'],\n",
        "            'precip_mode_if_rain': row['precip_mode_if_rain'],\n",
        "\n",
        "            # Wind speed\n",
        "            'wind_speed_min': row['wind_speed_min'],\n",
        "            'wind_speed_mean': row['wind_speed_mean'],\n",
        "            'wind_speed_max': row['wind_speed_max'],\n",
        "            'wind_speed_mode': row['wind_speed_mode'],\n",
        "\n",
        "            # Wind direction (fixed for the day)\n",
        "            'wind_dir_mean': row['wind_dir_mean']\n",
        "        })\n",
        "\n",
        "    # Create and return the merged DataFrame\n",
        "    future_climate_scenarios = pd.DataFrame(scenario_records)\n",
        "    return future_climate_scenarios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV3YGKahmAfR"
      },
      "source": [
        "#### __Monte Carlo Sampling Function__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCGtI6IcqB74"
      },
      "outputs": [],
      "source": [
        "def sample_uniform_monte(min_val, max_val, size=1):\n",
        "    \"\"\"\n",
        "    This function randomly a uniform distribution between min and max for every day.\n",
        "    It will repeat sampling nultiple times to generate a distribution of possible\n",
        "    outcomes - capturing uncertainty. Number of samples is set by n_simulations.\n",
        "    \"\"\"\n",
        "    return np.random.uniform(min_val, max_val, size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "177VIsJWn7LI"
      },
      "source": [
        "#### __Perform Model Forecast with Monte Carlo Simulation__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6cnBgjbmHP4"
      },
      "outputs": [],
      "source": [
        "def run_simulation(future_climate_scenarios, forecast_df_panel, model, sampler, n_simulations=100):\n",
        "    # Get the expected features from the model\n",
        "    try:\n",
        "        expected_features = model.feature_names_in_\n",
        "    except AttributeError:\n",
        "        raise ValueError(\"Model must have 'feature_names_in_' attribute (like most sklearn models).\")\n",
        "\n",
        "    # Extract which weather features are actually needed by the model\n",
        "    weather_columns = {\n",
        "        'avg_temp', 'min_temp', 'max_temp',\n",
        "        'avg_pressure', 'wind_speed', 'wind_dir', 'precip'\n",
        "    }\n",
        "    weather_needed = sorted(set(expected_features) & weather_columns)\n",
        "\n",
        "    if not weather_needed:\n",
        "        raise ValueError(\"Model does not appear to use any weather features — are you sure that's correct?\")\n",
        "\n",
        "    # Prepare to collect all simulation results and maximum temperature metrics\n",
        "    all_preds = []\n",
        "    temp_sims = []\n",
        "\n",
        "    for _ in range(n_simulations):\n",
        "        simulation_df = forecast_df_panel.copy()\n",
        "\n",
        "        # Sample only required weather columns\n",
        "        weather_by_date = {}\n",
        "        for _, row in future_climate_scenarios.iterrows():\n",
        "            date = row['date']\n",
        "            sampled = {}\n",
        "\n",
        "            if 'avg_temp' in weather_needed:\n",
        "                sampled['avg_temp'] = sampler(row['tavg_min'], row['tavg_max'])[0]\n",
        "            if 'min_temp' in weather_needed:\n",
        "                sampled['min_temp'] = sampler(row['tmin_min'], row['tmin_max'])[0]\n",
        "            if 'max_temp' in weather_needed:\n",
        "                sampled['max_temp'] = sampler(row['tmax_min'], row['tmax_max'])[0]\n",
        "            if 'avg_pressure' in weather_needed:\n",
        "                sampled['avg_pressure'] = sampler(row['avg_pressure_min'], row['avg_pressure_max'])[0]\n",
        "            if 'wind_speed' in weather_needed:\n",
        "                sampled['wind_speed'] = sampler(row['wind_speed_min'], row['wind_speed_max'])[0]\n",
        "            if 'wind_dir' in weather_needed:\n",
        "                sampled['wind_dir'] = (row['wind_dir_mean'] + np.random.normal(0, 5)) % 360\n",
        "            if 'precip' in weather_needed:\n",
        "                sampled['precip'] = sampler(row['precip_mean_if_rain'], row['precip_max_if_rain'])[0] \\\n",
        "                    if np.random.rand() < row['precip_rain_prob'] else 0.0\n",
        "\n",
        "            weather_by_date[date] = sampled\n",
        "\n",
        "        # Assign only required weather columns\n",
        "        for feature in weather_needed:\n",
        "            simulation_df[feature] = simulation_df['date'].map(lambda d: weather_by_date[d][feature])\n",
        "\n",
        "        # Align features for prediction\n",
        "        try:\n",
        "            X = simulation_df[expected_features]\n",
        "        except KeyError as e:\n",
        "            missing = list(set(expected_features) - set(simulation_df.columns))\n",
        "            raise ValueError(f\"Missing required features for model prediction: {missing}\")\n",
        "\n",
        "        preds = model.predict(X)\n",
        "        all_preds.append(preds)\n",
        "        temp_sims.append(simulation_df['max_temp'].values)\n",
        "\n",
        "    # Summarise predictions\n",
        "    preds_array = np.array(all_preds)\n",
        "    base_info = forecast_df_panel[['date', 'town/city']].reset_index(drop=True)\n",
        "    forecast_summary = base_info.copy()\n",
        "    forecast_summary['mean'] = preds_array.mean(axis=0)\n",
        "    forecast_summary['low_80'] = np.percentile(preds_array, 10, axis=0)\n",
        "    forecast_summary['up_80'] = np.percentile(preds_array, 90, axis=0)\n",
        "    forecast_summary['low_95'] = np.percentile(preds_array, 2.5, axis=0)\n",
        "    forecast_summary['up_95'] = np.percentile(preds_array, 97.5, axis=0)\n",
        "\n",
        "    temp_array = np.array(temp_sims)  # shape: (n_simulations, n_rows)\n",
        "\n",
        "    # Compute temperature stats\n",
        "    forecast_summary['forecasted_max_temp'] = temp_array.mean(axis=0)\n",
        "    forecast_summary['prob_exceed_28'] = (temp_array > 28).mean(axis=0)\n",
        "    forecast_summary['conditional_mean_temp'] = np.where(\n",
        "        (temp_array > 28).sum(axis=0) > 0,\n",
        "        temp_array * (temp_array > 28),\n",
        "        np.nan\n",
        "    ).sum(axis=0) / np.maximum((temp_array > 28).sum(axis=0), 1)\n",
        "\n",
        "    return forecast_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpKyg6VDovM7"
      },
      "source": [
        "#### __Plot Permutation Feature Importance for XGBoosted Model__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rT54e7zWwZbQ"
      },
      "outputs": [],
      "source": [
        "def tune_hgb_model_with_masking(df, y, params=None, n_splits=5, random_state=42):\n",
        "    \"\"\"\n",
        "    Train an HistGradientBoostingRegressor using TimeSeriesSplit with time-aware masking,\n",
        "    manually specifying hyperparameters (no grid search).\n",
        "\n",
        "    Parameters:\n",
        "    - df (DataFrame): Input dataframe with features and target. Must have a datetime index.\n",
        "    - y (str): Target variable name.\n",
        "    - params (dict, optional): Parameters for HistGradientBoostingRegressor.\n",
        "    - n_splits (int): Number of CV splits.\n",
        "    - random_state (int): Random seed.\n",
        "\n",
        "    Returns:\n",
        "    - model (HistGradientBoostingRegressor): Trained model on the full dataset.\n",
        "    - cv_rmse (float): Average RMSE across CV folds.\n",
        "    \"\"\"\n",
        "\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
        "        raise ValueError(\"DataFrame index must be datetime type.\")\n",
        "\n",
        "    feature_cols = [col for col in df.columns if col != y]\n",
        "    X = df[feature_cols]\n",
        "    y_series = df[y]\n",
        "\n",
        "    unique_dates = df.index.unique().sort_values()\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    params = params or {}\n",
        "    params['random_state'] = random_state\n",
        "\n",
        "    fold_rmse = []\n",
        "\n",
        "    for train_idx, test_idx in tscv.split(unique_dates):\n",
        "        train_dates = unique_dates[train_idx]\n",
        "        test_dates = unique_dates[test_idx]\n",
        "\n",
        "        train_mask = df.index.isin(train_dates)\n",
        "        test_mask = df.index.isin(test_dates)\n",
        "\n",
        "        X_train, y_train = X[train_mask], y_series[train_mask]\n",
        "        X_test, y_test = X[test_mask], y_series[test_mask]\n",
        "\n",
        "        model = HistGradientBoostingRegressor(**params)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        preds = model.predict(X_test)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "        fold_rmse.append(rmse)\n",
        "\n",
        "    cv_rmse = np.mean(fold_rmse)\n",
        "\n",
        "    # Train final model on full data\n",
        "    final_model = HistGradientBoostingRegressor(**params)\n",
        "    final_model.fit(X, y_series)\n",
        "\n",
        "    print(f\"Cross-validated RMSE: {cv_rmse:.4f}\")\n",
        "\n",
        "    return final_model, fold_rmse, cv_rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKa52VNjwZww"
      },
      "source": [
        "#### __Perform HGBoosted Model Regression with Timeseries Split__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BgyAwUHwL8f"
      },
      "outputs": [],
      "source": [
        "def evaluate_hgb_performance_with_best_model_masked(df, y_col, n_splits=None, best_model=None):\n",
        "    \"\"\"\n",
        "    Evaluates the HBBoost model using time series cross-validation and reports detailed metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): Full DataFrame containing features and target, indexed by datetime.\n",
        "    - y_col (str): Name of the target column.\n",
        "    - n_splits (int): Number of splits for TimeSeriesSplit.\n",
        "    - best_model: Fitted HGBoost model (e.g., from GridSearchCV).\n",
        "\n",
        "    Returns:\n",
        "    - best_model: The trained model after evaluation.\n",
        "    - fold_results (pd.DataFrame): Metrics for each fold.\n",
        "    - avg_results (dict): Average metrics across folds.\n",
        "    \"\"\"\n",
        "\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df.index):\n",
        "        raise ValueError(\"The DataFrame index must be of datetime type.\")\n",
        "\n",
        "    X = df.drop(columns=[y_col])\n",
        "    y = df[y_col]\n",
        "\n",
        "    unique_dates = df.index.unique().sort_values()\n",
        "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "    metrics_list = []\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(tscv.split(unique_dates)):\n",
        "        print(f\"\\nFold {fold + 1} of {n_splits}\")\n",
        "\n",
        "        train_dates = set(unique_dates[train_idx])\n",
        "        test_dates = set(unique_dates[test_idx])\n",
        "\n",
        "        train_mask = df.index.map(train_dates.__contains__)\n",
        "        test_mask = df.index.map(test_dates.__contains__)\n",
        "\n",
        "        X_train, X_test = X[train_mask], X[test_mask]\n",
        "        y_train, y_test = y[train_mask], y[test_mask]\n",
        "\n",
        "        best_model.fit(X_train, y_train)\n",
        "        y_pred = best_model.predict(X_test)\n",
        "\n",
        "        # Metrics\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mse)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        mape = np.mean(np.abs((y_test - y_pred) / np.where(y_test == 0, 1, y_test))) * 100\n",
        "        rmse_mae = rmse - mae\n",
        "        rmse_mae_cent = (rmse_mae / rmse) * 100\n",
        "\n",
        "        metrics_list.append({\n",
        "            'Fold': f'Fold {fold + 1}',\n",
        "            'MSE': mse,\n",
        "            'RMSE': rmse,\n",
        "            'R2': r2,\n",
        "            'MAE': mae,\n",
        "            'MAPE': mape,\n",
        "            'RMSE - MAE': rmse_mae,\n",
        "            'RMSE - MAE %': rmse_mae_cent\n",
        "        })\n",
        "\n",
        "    fold_results = pd.DataFrame(metrics_list)\n",
        "\n",
        "    avg_results = {\n",
        "        'Average MSE': fold_results['MSE'].mean(),\n",
        "        'Average RMSE': fold_results['RMSE'].mean(),\n",
        "        'Average R2': fold_results['R2'].mean(),\n",
        "        'Average MAE': fold_results['MAE'].mean(),\n",
        "        'Average MAPE': fold_results['MAPE'].mean(),\n",
        "        'Average RMSE - MAE': fold_results['RMSE - MAE'].mean(),\n",
        "        'Average RMSE - MAE %': fold_results['RMSE - MAE %'].mean()\n",
        "    }\n",
        "\n",
        "    # Output results\n",
        "    print(\"\\nFold-wise Performance:\")\n",
        "    print(fold_results)\n",
        "\n",
        "    print(\"\\nAverage Performance across all folds:\")\n",
        "    for k, v in avg_results.items():\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "    return best_model, fold_results, avg_results, X_test, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTtJ8gUNo5tB"
      },
      "source": [
        "#### __Plot Permutation Feature Importance for HGBoosted Model__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVvJsNs_1Pj0"
      },
      "outputs": [],
      "source": [
        "def plot_permutation_importance(model, X_test, y_test, n_repeats=10, random_state=seed):\n",
        "    \"\"\"\n",
        "    Computes and plots permutation feature importance.\n",
        "\n",
        "    Parameters:\n",
        "    - model: Trained model (already fitted).\n",
        "    - X_test: Validation features (DataFrame) from last fold.\n",
        "    - y_test: Validation target from last fold.\n",
        "    - n_repeats: Number of times to permute each feature.\n",
        "    - random_state: Seed for reproducibility.\n",
        "    \"\"\"\n",
        "\n",
        "    result = permutation_importance(\n",
        "        model, X_test, y_test,\n",
        "        n_repeats=n_repeats,\n",
        "        random_state=random_state,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "    importances = result.importances_mean\n",
        "    feature_names = X_test.columns\n",
        "\n",
        "\n",
        "    importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Importance': importances\n",
        "    })\n",
        "\n",
        "    # Separate positives and negatives\n",
        "    pos_df = importance_df[importance_df['Importance'] > 0].sort_values(by='Importance', ascending=False)\n",
        "    neg_df = importance_df[importance_df['Importance'] < 0].sort_values(by='Importance', ascending=True)\n",
        "    neg_df = neg_df.iloc[::-1]  # Flip so negative values are increasingly negative.\n",
        "\n",
        "    # Concatenate pos first, then neg\n",
        "    importance_df_sorted = pd.concat([pos_df, neg_df])\n",
        "\n",
        "    plt.figure(figsize=(10, max(6, len(feature_names) * 0.3)))\n",
        "\n",
        "    # Plot all bars at once\n",
        "    colors = ['tab:blue' if i > 0 else 'tab:orange' for i in importance_df_sorted['Importance']]\n",
        "    plt.barh(importance_df_sorted['Feature'], importance_df_sorted['Importance'], color=colors)\n",
        "\n",
        "    # Zero centered symmetric x-axis limits\n",
        "    max_val = max(abs(importance_df_sorted['Importance'].min()), importance_df_sorted['Importance'].max())\n",
        "    plt.xlim(-max_val * 1.1, max_val * 1.1)\n",
        "\n",
        "    plt.xlabel('Mean Permutation Importance')\n",
        "    plt.title('Permutation Feature Importance')\n",
        "    plt.gca().invert_yaxis()  # Flips the y-axis so first row is at top\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(importance_df_sorted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQNunhWzpQQJ"
      },
      "source": [
        "#### __Get Max Temperature Reference Normals, Daily Mean Deviation and Standard Deviation__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o-S8GCOpUtx"
      },
      "outputs": [],
      "source": [
        "def get_daily_tmax_normals_mean_std(df, station_id='08221', k_neighbors=5):\n",
        "    \"\"\"\n",
        "    Retrieve daily maximum temperature (tmax) reference normals and calculate\n",
        "    historical deviation statistics for the city using Meteostat data (1991–2020).\n",
        "\n",
        "    This function:\n",
        "    - Downloads daily historical tmax data for the station.\n",
        "    - Removes leap day (Feb 29) to standardize to a 365-day year.\n",
        "    - Imputes missing values using KNN imputation.\n",
        "    - Computes daily reference normals by averaging tmax across all years per day-of-year.\n",
        "    - Calculates mean and standard deviation of daily deviations (actual - normal).\n",
        "\n",
        "    Parameters:\n",
        "    - station_id (str): Meteostat station ID (default: _____)\n",
        "    - k_neighbors (int): Number of neighbors for KNN imputation (default: 5)\n",
        "\n",
        "    Returns:\n",
        "    - df (pd.DataFrame): DataFrame with a DatetimeIndex to determine the time window.\n",
        "    - ref_tmax_dict: dict mapping day_of_year (1–365) → average tmax\n",
        "    - mean_deviation: float, mean of all daily temperature deviations\n",
        "    - std_deviation: float, std dev of all daily temperature deviations\n",
        "    \"\"\"\n",
        "    # Fetch data from the area. The airport is considered to be\n",
        "    # a good representation of weather data for the Community.\n",
        "\n",
        "    # Check DataFrame has datetime index.\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        raise ValueError(\"Input df must have a DatetimeIndex.\")\n",
        "\n",
        "    # Derive dynamic time window for data collection.\n",
        "    end_date = df.index.min()\n",
        "    start_date = end_date - pd.DateOffset(years=30)\n",
        "    print(f\"Fetching normals from {start_date.date()} to {end_date.date()}\")\n",
        "\n",
        "    # Fetch data from Meteostat.\n",
        "    print(\"Fetching data...\")\n",
        "    data = Daily(station_id, start_date, end_date).fetch()\n",
        "    data = data[['tmax']].copy()\n",
        "\n",
        "    # Impute missing values using KNN.\n",
        "    if data['tmax'].isna().sum() > 0:\n",
        "        print(f\"Missing values before imputation: {data['tmax'].isna().sum()}\")\n",
        "        imputer = KNNImputer(n_neighbors=k_neighbors)\n",
        "        data[['tmax']] = imputer.fit_transform(data[['tmax']])\n",
        "        print(\"Missing values imputed.\")\n",
        "\n",
        "    # Select data which is NOT February 29. This will leave gaps for leap years\n",
        "    # which will need to be imputed in the dataset.\n",
        "    data = data[~((data.index.month == 2) & (data.index.day == 29))]\n",
        "\n",
        "    # Add day of year.\n",
        "    data['day_of_year'] = data.index.dayofyear\n",
        "\n",
        "    # Calculate daily normals\n",
        "    print(\"Calculating daily normals...\")\n",
        "    daily_normals = data.groupby('day_of_year')['tmax'].mean().reindex(range(1, 366))\n",
        "    daily_normals.name = 'ref_tmax'\n",
        "\n",
        "    # Merge normals back for deviation calculation.\n",
        "    data = data.merge(daily_normals, left_on='day_of_year', right_index=True)\n",
        "    data['temp_deviation'] = data['tmax'] - data['ref_tmax']\n",
        "\n",
        "    # Calculate statistics.\n",
        "    mean_deviation = data['temp_deviation'].mean()\n",
        "    std_deviation = data['temp_deviation'].std()\n",
        "\n",
        "    print(f\"Mean deviation: {mean_deviation:.2f}, Std deviation: {std_deviation:.2f}\")\n",
        "\n",
        "    return daily_normals.round(2).to_dict(), mean_deviation, std_deviation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYMGd_pFqWBI"
      },
      "source": [
        "#### __Prepare Forecasted Unit Sales with z-scores for Temperature Deviation from Normal__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FklyvNYWqWer"
      },
      "outputs": [],
      "source": [
        "def prepare_forecast_with_z_scores(forecast_df, ref_normals, mean_deviation, std_deviation, k_neighbors=5):\n",
        "    \"\"\"\n",
        "    Enhance a daily temperature forecast DataFrame by calculating deviation\n",
        "    from historical reference normals and computing z-scores and confidence\n",
        "    weights based on historical variability.\n",
        "\n",
        "    This function:\n",
        "    - Maps each forecasted date to its day-of-year (1–365).\n",
        "    - Joins reference normals (1991–2020 average tmax) based on day-of-year.\n",
        "    - Uses KNN imputation to fill in missing reference normals (e.g., Feb 29).\n",
        "    - Calculates temperature deviation: forecast_tmax - ref_tmax.\n",
        "    - Computes a z-score using provided historical mean and standard deviation\n",
        "      of deviation from normal.\n",
        "    - Assigns a confidence weighting to each forecast based on z-score thresholds\n",
        "      (lower z = higher confidence).\n",
        "\n",
        "    Parameters:\n",
        "    - forecast_df (pd.DataFrame): Must include 'date' (datetime64) and 'forecast_tmax' columns.\n",
        "    - ref_normals (dict): Mapping from day_of_year (1–365) to reference tmax.\n",
        "    - mean_deviation (float): Historical mean deviation from normal.\n",
        "    - std_deviation (float): Historical standard deviation of deviation.\n",
        "    - k_neighbors (int, optional): Number of neighbors to use for KNN imputation (default: 5).\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: Original forecast_df with the following added columns:\n",
        "        - 'day_of_year': Day of year (1–365)\n",
        "        - 'ref_tmax': Reference normal temperature\n",
        "        - 'temp_deviation': Forecast - normal\n",
        "        - 'z_score': Standardized deviation\n",
        "        - 'confidence_weight': Heuristic weighting. A value between 0 (no confidence)\n",
        "           and 1 (very confident), based on the z-score\n",
        "    \"\"\"\n",
        "    df = forecast_df.copy()\n",
        "\n",
        "    # Ensure the index is datetime\n",
        "    if not isinstance(df.index, pd.DatetimeIndex):\n",
        "        raise ValueError(\"DataFrame must have a DatetimeIndex.\")\n",
        "\n",
        "    # Generate the day of year for reference mapping.\n",
        "    df['day_of_year'] = df.index.dayofyear\n",
        "\n",
        "    # Map reference normals.\n",
        "    df['ref_tmax'] = df['day_of_year'].map(ref_normals)\n",
        "\n",
        "    # Impute any missing ref_tmax (e.g., Feb 29) using KNN.\n",
        "    if df['ref_tmax'].isna().sum() > 0:\n",
        "        imputer = KNNImputer(n_neighbors=k_neighbors)\n",
        "        df[['ref_tmax']] = imputer.fit_transform(df[['ref_tmax']])\n",
        "\n",
        "    # Calculate deviation from reference normal.\n",
        "    df['temp_deviation'] = df['forecasted_max_temp'] - df['ref_tmax']\n",
        "\n",
        "    # Calculate the Z-score (how far the deviation is in std devs).\n",
        "    df['z_score'] = (df['temp_deviation'] - mean_deviation) / std_deviation\n",
        "\n",
        "    # Confidence weighting. Corresponds to a roughly normal distribution where\n",
        "    # approx. 68% values lie within 1 std dev, 95% within 2 and 99% within 3.\n",
        "    # Can be adjusted to be more aggressive.\n",
        "    def confidence_weight(z):\n",
        "        z = abs(z)\n",
        "        if z < 1:\n",
        "            return 0.7\n",
        "        elif z < 2:\n",
        "            return 0.5\n",
        "        elif z < 3:\n",
        "            return 0.3\n",
        "        else:\n",
        "            return 0.2\n",
        "\n",
        "    df['confidence_weight'] = df['z_score'].apply(confidence_weight)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wajrH4e6qcB1"
      },
      "source": [
        "#### __Adjust Ratios Based on Confidence Weighting and Temperature Thresholds and Calculate Revenue__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfSHXPx7qeVf"
      },
      "outputs": [],
      "source": [
        "def adjust_ratios_calc_revenue(df, base_unit2_ratio=0.02, max_unit2_ratio=0.10,\n",
        "                               unit2_price=1.89, unit_price=1.49,\n",
        "                               unit2_margin=0.4, unit_margin=0.2,\n",
        "                               prob_threshold=0.5, temp_threshold=28):\n",
        "    \"\"\"\n",
        "    Adjust premium (unit2) vs standard (unit) product ratio based on:\n",
        "      - Probability day exceeds 28°C (prob_exceed_28)\n",
        "      - Conditional mean temperature (conditional_mean_temp)\n",
        "      - Forecast uncertainty (z_score)\n",
        "      - Event days affecting unit demand\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): Must have columns:\n",
        "        'prob_exceed_28', 'conditional_mean_temp', 'z_score', 'is_event_day',\n",
        "        'up_80', 'up_95'\n",
        "    - base_unit2_ratio: Default premium product ratio when cool\n",
        "    - max_unit2_ratio: Max premium ratio for hot days\n",
        "    - unit2_price, unit_price, unit2_margin, unit_margin: Pricing and margins\n",
        "    - prob_threshold: Probability cutoff to start adjusting above base ratio\n",
        "    - temp_threshold: Temperature threshold for boosting signal\n",
        "\n",
        "    Returns:\n",
        "    - df with adjusted ratios, revenue and profit columns added\n",
        "    \"\"\"\n",
        "\n",
        "    df = df.copy()\n",
        "\n",
        "    # Vectorized selection of sold unit CI band based on whether an event occurs or not.\n",
        "    df['total_units'] = np.where(df['is_event_day'], df['up_95'], df['up_80'])\n",
        "\n",
        "    # Compute base heat signal from probability (scaled 0 to 1).\n",
        "    df['heat_signal'] = np.clip((df['prob_exceed_28'] - prob_threshold) / (1 - prob_threshold), 0, 1)\n",
        "\n",
        "    # Temperature boost.\n",
        "    df['temp_boost'] = np.clip((df['conditional_mean_temp'] - temp_threshold) / 10, 0, 1)\n",
        "    df['temp_boost'] = df['temp_boost'].fillna(1.0)\n",
        "\n",
        "    # Combine heat signal with temp boost.\n",
        "    df['heat_signal'] *= df['temp_boost']\n",
        "\n",
        "    # Dampening for high uncertainty forecasts.\n",
        "    df.loc[df['z_score'].abs() > 3, 'heat_signal'] *= 0.7\n",
        "\n",
        "    # Below-threshold probability gets no heat signal, keeps base ratio of 2%.\n",
        "    df.loc[df['prob_exceed_28'] < prob_threshold, 'heat_signal'] = 0\n",
        "\n",
        "    # Adjusted ratios.\n",
        "    df['adjusted_unit2_ratio'] = base_unit2_ratio + (max_unit2_ratio - base_unit2_ratio) * df['heat_signal']\n",
        "    df['adjusted_unit2_ratio'] = df['adjusted_unit2_ratio'].clip(lower=base_unit2_ratio, upper=max_unit2_ratio)\n",
        "    df['adjusted_unit_ratio'] = 1 - df['adjusted_unit2_ratio']\n",
        "\n",
        "    # Revenue and profit calculations.\n",
        "    df['unit2_revenue'] = df['adjusted_unit2_ratio'] * df['total_units'] * unit2_price\n",
        "    df['unit_revenue'] = df['adjusted_unit_ratio'] * df['total_units'] * unit_price\n",
        "    df['total_revenue'] = df['unit2_revenue'] + df['unit_revenue']\n",
        "    df['expected_profit'] = (\n",
        "        df['adjusted_unit2_ratio'] * df['total_units'] * unit2_price * unit2_margin +\n",
        "        df['adjusted_unit_ratio'] * df['total_units'] * unit_price * unit_margin\n",
        "    )\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk52sLjjnSRV"
      },
      "source": [
        "## 🔹 Target and Predictor Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "883a5f5d"
      },
      "source": [
        "#### __Import and Validate filtered_sales_temp_holidays_and_matches.csv Dataset for Modelling__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cea9c4af"
      },
      "outputs": [],
      "source": [
        "# Locate the dataset in Google Drive.\n",
        "filepath_sales_events_temp = '/content/drive/MyDrive/Employer Project/Datasets/Production Data/filtered_sales_temp_hols_fmatches/final_events_clean.csv'\n",
        "\n",
        "# Load the dataset.\n",
        "sales_events_temp = pd.read_csv(filepath_sales_events_temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99e3a56c"
      },
      "outputs": [],
      "source": [
        "# View dataframe.\n",
        "sales_events_temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f4712b6"
      },
      "outputs": [],
      "source": [
        "# Determine metadata and accurate import.\n",
        "sales_events_temp.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2d7ad3d"
      },
      "source": [
        "#### __Address missing rows for 0 sales per sales location__\n",
        "In the sales_events_temp dataset, each sales_location represents a unique location, and sold_units reflects daily sales. We identified that some sales_locations lack records for certain dates—not due to missing data, but because no sales occurred.\n",
        "\n",
        "To create a consistent time series, we assume that missing rows indicate zero sales. We generated all combinations of sales_location and date, filling in missing rows with sold_units = 0. Other fields in these rows are populated based on existing entries for the same date to maintain consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9311166"
      },
      "outputs": [],
      "source": [
        "# Change date column to datetime format\n",
        "sales_events_temp['date'] = pd.to_datetime(sales_events_temp['date'])\n",
        "\n",
        "# Generate all dates in 2021 and 2022\n",
        "all_dates = pd.date_range(start='2021-01-01', end='2022-12-31')\n",
        "\n",
        "# Extract unique dates for 2021 and 2022\n",
        "existing_dates = sales_events_temp[\n",
        "    sales_events_temp['date'].dt.year.isin([2021, 2022])\n",
        "]['date'].dt.normalize().unique()\n",
        "\n",
        "# Convert to datetime index for comparison\n",
        "existing_dates = pd.to_datetime(existing_dates)\n",
        "\n",
        "# Find missing dates\n",
        "missing_dates = all_dates.difference(existing_dates)\n",
        "\n",
        "# Display the result\n",
        "print(\"Missing dates in 2022:\")\n",
        "print(missing_dates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d684934"
      },
      "outputs": [],
      "source": [
        "# Define all valid (non-missing) dates in 2022\n",
        "all_dates = pd.date_range(start='2021-01-01', end='2022-12-31')\n",
        "existing_dates = pd.to_datetime(\n",
        "    sales_events_temp[\n",
        "        sales_events_temp['date'].dt.year.isin([2021, 2022])\n",
        "    ]['date'].dt.normalize().unique()\n",
        ")\n",
        "\n",
        "missing_dates = all_dates.difference(existing_dates)\n",
        "valid_dates = pd.DatetimeIndex(existing_dates)\n",
        "\n",
        "# Get all unique sales station codes\n",
        "stations = sales_events_temp['sales_location'].unique()\n",
        "\n",
        "# Create full grid of station × date\n",
        "full_grid = pd.MultiIndex.from_product(\n",
        "    [stations, valid_dates],\n",
        "    names=['sales_location', 'date']\n",
        ").to_frame(index=False)\n",
        "\n",
        "# Merge with actual 2022 data\n",
        "existing_data = sales_events_temp[sales_events_temp['date'].dt.year.isin([2021, 2022])]\n",
        "merged = full_grid.merge(\n",
        "    existing_data,\n",
        "    on=['sales_location', 'date'],\n",
        "    how='left')\n",
        "\n",
        "# Fill missing `sold_units` with 0\n",
        "merged['sold_units'] = merged['sold_units'].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5eb896f"
      },
      "outputs": [],
      "source": [
        "# Define columns that should be filled based on 'date'\n",
        "date_based_columns = [\n",
        "    'day', 'month', 'year', 'month_text', 'day_of_week', 'season',\n",
        "    'avg_temp', 'min_temp', 'max_temp', 'precip', 'wind_dir',\n",
        "    'wind_speed', 'avg_pressure', 'holiday_name',\n",
        "    'sport_match', 'sport1_match',\n",
        "    'is_holiday', 'is_sport', 'is_sport1', 'is_match', 'event_category'\n",
        "]\n",
        "\n",
        "# Fill missing values by mapping values from other rows with the same date\n",
        "# Step 1: Create a lookup table with one row per date\n",
        "date_lookup = merged[~merged[date_based_columns].isna().all(axis=1)].groupby('date')[date_based_columns].first()\n",
        "\n",
        "# Merge this back to the original DataFrame on 'date' to fill missing values\n",
        "merged_filled = merged.drop(columns=date_based_columns).merge(\n",
        "    date_lookup, how='left', on='date'\n",
        ")\n",
        "\n",
        "# Done — check if missing values are resolved\n",
        "missing_after = merged_filled[date_based_columns].isna().sum()\n",
        "print(\"Missing values after filling based on date:\")\n",
        "print(missing_after)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaa591d8"
      },
      "outputs": [],
      "source": [
        "# List of columns to fill based on sales_location\n",
        "station_info_cols = ['province', 'town/city', 'postcode', 'company_code', 'sales_location']\n",
        "\n",
        "# Fill missing values by group (based on sales_location)\n",
        "merged_filled[station_info_cols] = (\n",
        "    merged_filled\n",
        "    .groupby('sales_location')[station_info_cols]\n",
        "    .transform(lambda group: group.ffill().bfill())\n",
        ")\n",
        "print(merged_filled[station_info_cols].isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d3e9736"
      },
      "outputs": [],
      "source": [
        "# Convert columns in merged_filled to match the data types of sales_events_temp\n",
        "\n",
        "# Adjust int64 columns (e.g., company_code, sales_location, etc.)\n",
        "int_columns = ['company_code', 'postcode', 'sales_location', 'day', 'month', 'year', 'sold_units']  # example of int columns\n",
        "merged_filled[int_columns] = merged_filled[int_columns].astype('int64')\n",
        "\n",
        "# Adjust float64 columns (e.g., avg_temp, min_temp, max_temp, etc.)\n",
        "float_columns = ['avg_temp', 'min_temp', 'max_temp', 'precip', 'wind_dir', 'wind_speed', 'avg_pressure']\n",
        "merged_filled[float_columns] = merged_filled[float_columns].astype('float64')\n",
        "\n",
        "# Adjust object columns (e.g., province, town/city, etc.)\n",
        "object_columns = ['province', 'town/city', 'sales_location', 'month_text', 'day_of_week', 'season', 'holiday_name', 'event_category']\n",
        "merged_filled[object_columns] = merged_filled[object_columns].astype('object')\n",
        "\n",
        "# Adjust boolean columns\n",
        "boolean_columns = ['is_holiday', 'is_sport', 'is_sport1', 'is_match']\n",
        "merged_filled[boolean_columns] = merged_filled[boolean_columns].astype('bool')\n",
        "\n",
        "# Check the updated data types in merged_filled\n",
        "sales_events_temp = merged_filled\n",
        "sales_events_temp.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b580a57"
      },
      "source": [
        "#### __Address Missing Weather Variables__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22d2e113"
      },
      "outputs": [],
      "source": [
        "# Fill missing avg_temp values.\n",
        "sales_events_temp['avg_temp'] = sales_events_temp['avg_temp'].fillna(\n",
        "    (\n",
        "        sales_events_temp['max_temp'] + sales_events_temp['min_temp']\n",
        "    ) / 2\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60b-BRniMLHC"
      },
      "source": [
        "Average temperature calculated as the maximum + minimum / 2 is a commonly used method to fill missing average temperatures. To ensure consistency with how missing average temperatures are typically resolved, this technique has been utilised here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b70f8852"
      },
      "outputs": [],
      "source": [
        "# Select columns which require filling.\n",
        "cols = ['wind_dir', 'wind_speed', 'avg_pressure']\n",
        "original_data = sales_events_temp[cols]\n",
        "\n",
        "# Perform KNN imputation.\n",
        "imputer = KNNImputer(n_neighbors=5)  # Use k=5 for KNN imputation.\n",
        "imputed_weather_values = pd.DataFrame(imputer.fit_transform(original_data), columns=cols, index=original_data.index)\n",
        "\n",
        "# Round imputed DataFrame.\n",
        "imputed_weather_values = imputed_weather_values.round(1)\n",
        "\n",
        "# Use fillna to only update missing values.\n",
        "for col in cols:\n",
        "    sales_events_temp[col] = original_data[col].fillna(imputed_weather_values[col])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f2f58c4"
      },
      "source": [
        "KNN imputation is commonly used to fill missing weather data. It leverages the similarities and temporal nature of weather events to impute the missing values within the dataset and is generally found to be more accurate than mean or median. A k of 5 is chosen as the missing values comprise approximately 0.5% of the dataset and a k of five is assumed to roughly capture the short-term weather patterns and gradual changes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f56d0bcb"
      },
      "source": [
        "#### __Geocode Postcodes to Generate Latitude and Longitude Values for Modelling__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5388445b"
      },
      "outputs": [],
      "source": [
        "# Check for unique postcodes in the dataset.\n",
        "postcodes_coord = pd.DataFrame(sales_events_temp['postcode'].drop_duplicates())\n",
        "\n",
        "postcodes_coord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cba82b62"
      },
      "outputs": [],
      "source": [
        "# Retrieve the lat and lon of for each postcode.\n",
        "postcodes_coord[['latitude', 'longitude']] = postcodes_coord['postcode'].apply(\n",
        "    lambda x: pd.Series(get_coordinates_from_postcode(x))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "485b8b74"
      },
      "outputs": [],
      "source": [
        "# Sense-check coordinates against postcodes.\n",
        "postcodes_coord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4b0c679"
      },
      "outputs": [],
      "source": [
        "# Check for null coordinate values.\n",
        "postcodes_coord.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb2ff625"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary: {postcode: (lat, lon)} from postcodes_coord DataFrame.\n",
        "# Set 'postal_code' as the index.\n",
        "postcodes_coord = postcodes_coord.set_index('postcode')\n",
        "\n",
        "# Select latitude and longitude columns and convert to a tuple.\n",
        "postcodes_coord = postcodes_coord[['latitude', 'longitude']].apply(tuple, axis=1)\n",
        "\n",
        "# Convert the Series to a dictionary.\n",
        "postcode_coords_dict = postcodes_coord.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acf0bd91"
      },
      "outputs": [],
      "source": [
        "# Copy the sales_events_temp DataFrame before geocoding.\n",
        "sales_events_temp_geo = sales_events_temp.copy()\n",
        "\n",
        "# Map lat and lon to sales_events_temp DataFrame.\n",
        "sales_events_temp_geo[['latitude', 'longitude']] = sales_events_temp_geo['postcode'].map(postcode_coords_dict).apply(pd.Series)\n",
        "\n",
        "sales_events_temp_geo.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e37671c1"
      },
      "outputs": [],
      "source": [
        "# Define new column order following logical grouping.\n",
        "column_order = [\n",
        "    \"province\", \"town/city\", \"postcode\", \"latitude\", \"longitude\",\n",
        "    \"company_code\", \"sales_location\", \"sales_location\",\n",
        "    \"date\", \"year\", \"month\", \"month_text\", \"day\", \"day_of_week\", \"season\",\n",
        "    \"sold_units\",\n",
        "    \"avg_temp\", \"min_temp\", \"max_temp\", \"precip\",\n",
        "    \"wind_dir\", \"wind_speed\", \"avg_pressure\",\n",
        "    \"holiday_name\", \"is_holiday\",\n",
        "    \"sport_match\", \"is_sport\", \"sport1_match\", \"is_sport1\", \"is_match\"\n",
        "]\n",
        "\n",
        "# Reorder DataFrame columns.\n",
        "sales_events_temp_geo = sales_events_temp_geo[column_order]\n",
        "\n",
        "# View DataFrame.\n",
        "sales_events_temp_geo.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e73c8bf5"
      },
      "outputs": [],
      "source": [
        "# Download the geocoded dataset as a CSV.\n",
        "sales_events_temp_geo.to_csv('/content/drive/MyDrive/Employer Project/Datasets/Production Data/filtered_sales_temp_hols_fmatches/sales_events_temp_geocoded.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb24881c"
      },
      "source": [
        "#### __Import and Validate Max Temp Reference Normals__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4004f041"
      },
      "source": [
        "##### __Use Meteostat to Generate and Store Daily Rolling Reference for the metropolitan area 2021 and 2022__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0e62d79"
      },
      "outputs": [],
      "source": [
        "# Ensure date column is datetime.\n",
        "sales_events_temp_geo['date'] = pd.to_datetime(sales_events_temp_geo['date'])\n",
        "\n",
        "# Create a day-of-year column (1 to 365).\n",
        "sales_events_temp_geo['day_of_year'] = sales_events_temp_geo['date'].dt.dayofyear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bd5abacf"
      },
      "outputs": [],
      "source": [
        "# Retrieve reference normal values for 2021 and 2022.\n",
        "normals_2021 = get_daily_temp_normals(start_year=2021, end_year=2021)\n",
        "normals_2022 = get_daily_temp_normals(start_year=2022, end_year=2022)\n",
        "\n",
        "# Filter the dataset for 2021 and 2022.\n",
        "sales_events_2021 = sales_events_temp_geo[sales_events_temp_geo['year'] == 2021]\n",
        "sales_events_2022 = sales_events_temp_geo[sales_events_temp_geo['year'] == 2022]\n",
        "\n",
        "# Map normals for 2021.\n",
        "sales_events_2021['ref_tmax'] = sales_events_2021['day_of_year'].map(normals_2021['ref_tmax'])\n",
        "sales_events_2021['ref_tmin'] = sales_events_2021['day_of_year'].map(normals_2021['ref_tmin'])\n",
        "sales_events_2021['ref_tavg'] = sales_events_2021['day_of_year'].map(normals_2021['ref_tavg'])\n",
        "\n",
        "# Map normals for 2022.\n",
        "sales_events_2022['ref_tmax'] = sales_events_2022['day_of_year'].map(normals_2022['ref_tmax'])\n",
        "sales_events_2022['ref_tmin'] = sales_events_2022['day_of_year'].map(normals_2022['ref_tmin'])\n",
        "sales_events_2022['ref_tavg'] = sales_events_2022['day_of_year'].map(normals_2022['ref_tavg'])\n",
        "\n",
        "# Combine both years back together.\n",
        "sales_events_temp_geo = pd.concat([sales_events_2021, sales_events_2022])\n",
        "\n",
        "# Check if any missing values exist.\n",
        "if sales_events_temp_geo[['ref_tmax', 'ref_tmin', 'ref_tavg']].isna().sum().sum() > 0:\n",
        "    print(f\"Missing values detected, applying KNN imputation...\")\n",
        "\n",
        "    # Prepare for imputation.\n",
        "    for var in ['ref_tmax', 'ref_tmin', 'ref_tavg']:\n",
        "        if sales_events_temp_geo[var].isna().sum() > 0:\n",
        "            original_values = sales_events_temp_geo[[var]].copy()\n",
        "\n",
        "            # Perform KNN imputation\n",
        "            imputer = KNNImputer(n_neighbors=5)\n",
        "            imputed_values = pd.DataFrame(imputer.fit_transform(original_values),\n",
        "                                          columns=[var],\n",
        "                                          index=original_values.index)\n",
        "\n",
        "            # Round and update missing values\n",
        "            sales_events_temp_geo[var] = original_values[var].fillna(imputed_values[var].round(2))\n",
        "\n",
        "    print(\"Missing values imputed successfully.\")\n",
        "else:\n",
        "    print(\"No missing values found.\")\n",
        "\n",
        "# Sense check the DataFrame\n",
        "sales_events_temp_geo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0a77efe5"
      },
      "outputs": [],
      "source": [
        "# Look for missing reference temperatures.\n",
        "sales_events_temp_geo.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cf869c9"
      },
      "source": [
        "##### __Finalise DataFrame for Initial Visualisation__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5168865d"
      },
      "outputs": [],
      "source": [
        "# Drop uneccessary columns for future modelling.\n",
        "sales_hols_reftemp_geo = sales_events_temp_geo.drop(\n",
        "    columns=[\n",
        "        'holiday_name',\n",
        "        'sport_match',\n",
        "        'sport1_match'\n",
        "    ]\n",
        ")\n",
        "\n",
        "sales_hols_reftemp_geo.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3d7e916"
      },
      "outputs": [],
      "source": [
        "# Define new column order following logical grouping.\n",
        "column_order = [\n",
        "    \"province\", \"town/city\", \"postcode\", \"latitude\", \"longitude\",\n",
        "    \"company_code\", \"sales_location\", \"sales_location\",\n",
        "    \"date\", \"year\", \"month\", \"month_text\", \"day\", \"day_of_year\", \"day_of_week\",\n",
        "    \"season\", \"sold_units\", \"avg_temp\", \"min_temp\", \"max_temp\",\n",
        "    \"ref_tavg\", \"ref_tmin\", \"ref_tmax\", \"precip\", \"wind_dir\", \"wind_speed\",\n",
        "    \"avg_pressure\", \"is_holiday\", \"is_sport\", \"is_sport1\", \"is_match\"\n",
        "]\n",
        "\n",
        "# Reorder DataFrame columns.\n",
        "sales_hols_reftemp_geo = sales_hols_reftemp_geo[column_order]\n",
        "\n",
        "# View DataFrame.\n",
        "sales_hols_reftemp_geo.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e9ebb97"
      },
      "outputs": [],
      "source": [
        "# Download the geocoded dataset as a CSV.\n",
        "sales_hols_reftemp_geo.to_csv('/content/drive/MyDrive/Employer Project/Datasets/Production Data/filtered_sales_temp_hols_fmatches/sales_hols_reftemp_geocoded.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f395f7ae"
      },
      "source": [
        "#### __Variable Correlation__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06d635af"
      },
      "outputs": [],
      "source": [
        "# Ensure that the appropriate variables are categorical vs. numerical.\n",
        "# Convert postcode to string to preserve any leading zeros or alphanumeric codes.\n",
        "sales_hols_reftemp_geo['postcode'] = sales_hols_reftemp_geo['postcode'].astype(str)\n",
        "\n",
        "# Convert date to datetime format.\n",
        "sales_hols_reftemp_geo['date'] = pd.to_datetime(sales_hols_reftemp_geo['date'])\n",
        "\n",
        "# Convert company_code and sales_location to string (identifiers should be stored as strings).\n",
        "sales_hols_reftemp_geo['company_code'] = sales_hols_reftemp_geo['company_code'].astype(str)\n",
        "sales_hols_reftemp_geo['sales_location'] = sales_hols_reftemp_geo['sales_location'].astype(str)\n",
        "\n",
        "# Verify the changes by checking the metadata.\n",
        "sales_hols_reftemp_geo.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a91231a"
      },
      "outputs": [],
      "source": [
        "# Check high-level correlation between variables.\n",
        "corr_matrix = sales_hols_reftemp_geo.corr(numeric_only=True)\n",
        "\n",
        "# Create the heatmap for correlation\n",
        "plt.figure(figsize=(16, 16))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0g-RmASYSU8"
      },
      "source": [
        "There are many limitations to this correlation plot, we know this is largely looking at linear correlations, and we can see the temperature/climate variables demonstrate stronger correlation than other variables which were demonstrated to at least visually have a relationship to the target variable. This plot should therefore be view skeptically. It does not necessarily demonstrate the complexity of the relationships seen visually. There is likely a lot of colinearity between between climate, holidays, and other temporal variables. High heat occurs on certain days of the year, seasons and months and there is even a temporal relationship to sports events which also occurs cyclically. It is very doubtful that these complex relationships could be captured accurately by a simpler model such as multiple linear regression or even ARIMA."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d1a9bd2"
      },
      "source": [
        "##### __Refine Variables__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0ac5add"
      },
      "outputs": [],
      "source": [
        "# Clean up redundant variables. Some variables will be removed based on\n",
        "# performance of models.\n",
        "sales_modelling_variables = sales_hols_reftemp_geo.drop(columns=['postcode',\n",
        "                                                                 'month_text',\n",
        "                                                                 ])\n",
        "\n",
        "sales_modelling_variables.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d264c524"
      },
      "source": [
        "##### __Encode Required Variables__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff150c7b"
      },
      "outputs": [],
      "source": [
        "# Ensure order for season variable.\n",
        "season_order = ['Winter', 'Spring', 'Summer', 'Autumn']\n",
        "\n",
        "sales_modelling_variables['season'] = pd.Categorical(sales_modelling_variables['season'], categories=season_order, ordered=True)\n",
        "\n",
        "# One-hot encode required variables. None have inherent ordinality.\n",
        "sales_modelling_vars_one_hot = one_hot_encode_columns(sales_modelling_variables, ['season'], prefix=True, drop_first=True, keep_original=True)\n",
        "\n",
        "# Sense check DataFrame.\n",
        "sales_modelling_vars_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "927c0bfb"
      },
      "outputs": [],
      "source": [
        "# Ensure cyclical variables are ordered and numeric.\n",
        "# Ensure order for categorical day of week variable.\n",
        "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "sales_modelling_vars_one_hot['day_of_week'] = pd.Categorical(sales_modelling_vars_one_hot['day_of_week'], categories=day_order, ordered=True)\n",
        "\n",
        "\n",
        "# List of cyclical variables and their expected index base (0 or 1)\n",
        "cyclical_variables_info = {\n",
        "    'month': {'period': 12, 'index_base': 1},\n",
        "    'day_of_week': {'period': 7, 'index_base': 0},\n",
        "    'day_of_year': {'period': 365, 'index_base': 1},\n",
        "}\n",
        "\n",
        "# Make a copy of the DataFrame to work safely.\n",
        "sales_modelling_variables_encoded = sales_modelling_vars_one_hot.copy()\n",
        "\n",
        "for col, info in cyclical_variables_info.items():\n",
        "    # Convert categorical or object columns to codes.\n",
        "    if sales_modelling_variables_encoded[col].dtype.name in ['category', 'object']:\n",
        "        sales_modelling_variables_encoded[col] = pd.Categorical(sales_modelling_variables_encoded[col]).codes\n",
        "\n",
        "    # Force numeric, coerce errors to NaN for safety.\n",
        "    sales_modelling_variables_encoded[col] = pd.to_numeric(sales_modelling_variables_encoded[col], errors='coerce')\n",
        "\n",
        "    # Ensure 0-based or 1-based indexing as specified.\n",
        "    if info['index_base'] == 1 and sales_modelling_variables_encoded[col].min() == 0:\n",
        "        sales_modelling_variables_encoded[col] = sales_modelling_variables_encoded[col] + 1  # Shift to 1-based where required.\n",
        "\n",
        "# Sense check the DataFrame.\n",
        "sales_modelling_variables_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60510472"
      },
      "outputs": [],
      "source": [
        "# Define periods for cyclical encoding.\n",
        "cyclical_columns = {\n",
        "    'month': 12,\n",
        "    'day_of_week': 7,\n",
        "    'day_of_year': 365\n",
        "}\n",
        "\n",
        "# Apply cyclical encoding.\n",
        "sales_modelling_encoded = add_cyclical_encoding(sales_modelling_variables_encoded, columns_periods=cyclical_columns, drop_original=False)\n",
        "\n",
        "# View dataframe.\n",
        "sales_modelling_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfdc1622"
      },
      "outputs": [],
      "source": [
        "# Sense check DataFrame.\n",
        "sales_modelling_encoded.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e000623e"
      },
      "outputs": [],
      "source": [
        "# Set logical order for variables.\n",
        "column_order = [\n",
        "    'province',\n",
        "    'town/city',\n",
        "    'latitude',\n",
        "    'longitude',\n",
        "    'company_code',\n",
        "    'sales_location',\n",
        "    'sold_units',\n",
        "    'date',\n",
        "    'year',\n",
        "    'month',\n",
        "    'month_sin',\n",
        "    'month_cos',\n",
        "    'day',\n",
        "    'day_of_week',\n",
        "    'day_of_week_sin',\n",
        "    'day_of_week_cos',\n",
        "    'day_of_year',\n",
        "    'day_of_year_sin',\n",
        "    'day_of_year_cos',\n",
        "    'season_Spring',\n",
        "    'season_Summer',\n",
        "    'season_Autumn',\n",
        "    'avg_temp',\n",
        "    'min_temp',\n",
        "    'max_temp',\n",
        "    'ref_tavg',\n",
        "    'ref_tmin',\n",
        "    'ref_tmax',\n",
        "    'precip',\n",
        "    'wind_dir',\n",
        "    'wind_speed',\n",
        "    'avg_pressure',\n",
        "    'is_holiday',\n",
        "    'is_sport',\n",
        "    'is_sport1',\n",
        "    'is_match'\n",
        "]\n",
        "\n",
        "# Reorder DataFrame columns.\n",
        "sales_modelling_encoded  = sales_modelling_encoded [column_order]\n",
        "\n",
        "# View DataFrame.\n",
        "sales_modelling_encoded.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7380a008"
      },
      "outputs": [],
      "source": [
        "# Save to csv.\n",
        "sales_modelling_encoded.to_csv('/content/drive/MyDrive/Employer Project/Datasets/Production Data/filtered_sales_temp_hols_fmatches/sales_modelling_encoded.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a5f3956"
      },
      "source": [
        "##### __Revisit Correlation with Encoded Variables__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b2386919"
      },
      "outputs": [],
      "source": [
        "# Subset sales_modelling_encoded for correlation visulisation.\n",
        "sales_modelling_encoded_sub = sales_modelling_encoded[[\n",
        "    'latitude', 'longitude', 'sold_units', 'month_sin', 'month_cos',\n",
        "    'day_of_week_sin', 'day_of_week_cos', 'day_of_year_sin', 'day_of_year_cos',\n",
        "    'season_Spring', 'season_Summer', 'season_Autumn',\n",
        "    'avg_temp', 'min_temp', 'max_temp', 'precip',\n",
        "    'wind_dir', 'wind_speed', 'avg_pressure', 'is_holiday', 'is_sport',\n",
        "    'is_sport1'\n",
        "]]\n",
        "\n",
        "# Check high-level correlation between variables.\n",
        "corr_matrix = sales_modelling_encoded_sub.corr(numeric_only=True)\n",
        "\n",
        "# Create the heatmap for correlation\n",
        "plt.figure(figsize=(16, 16))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FadvOnEZyQz"
      },
      "source": [
        "Encoding temporal variables cyclically has improved the visible correlation for these variables. This demonstrates that perhaps the relationship between these temporal variables and the sold units is best represented this way, capturing the true cyclical nature of time (which will be important for the model), but also that again the relationships are unlikely to be modelled well with a simpler model that fails to capture the complex and intertwined nature of all the predictor variables. As we can see some of these relationships established here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9133399b"
      },
      "outputs": [],
      "source": [
        "# Convert boolean columns to integers.\n",
        "bool_cols = sales_modelling_encoded_sub.select_dtypes(include='bool').columns\n",
        "sales_modelling_encoded_sub[bool_cols] = sales_modelling_encoded_sub[bool_cols].astype(int)\n",
        "\n",
        "# Calculate the VIF for each variable.\n",
        "vif_data = pd.DataFrame()\n",
        "features = sales_modelling_encoded_sub.drop('sold_units', axis=1)\n",
        "vif_data[\"feature\"] = features.columns\n",
        "vif_data[\"VIF\"] = [\n",
        "    variance_inflation_factor(features.values, i)\n",
        "    for i in range(len(features.columns))\n",
        "]\n",
        "\n",
        "vif_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6eec30c"
      },
      "outputs": [],
      "source": [
        "# Drop potentially redundant columns.\n",
        "sales_modelling_encoded_vif = sales_modelling_encoded_sub[[\n",
        "    'latitude', 'longitude', 'month_sin', 'month_cos',\n",
        "    'day_of_week_sin', 'day_of_week_cos', 'day_of_year_sin', 'day_of_year_cos',\n",
        "    'season_Spring', 'season_Summer', 'season_Autumn',\n",
        "    'max_temp', 'precip', 'wind_dir', 'wind_speed', 'is_holiday', 'is_sport',\n",
        "    'is_sport1'\n",
        "]]\n",
        "\n",
        "# Create a DataFrame to capture VIF values.\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = sales_modelling_encoded_vif.columns\n",
        "\n",
        "# Convert boolean columns to integers.\n",
        "bool_cols = sales_modelling_encoded_vif.select_dtypes(include='bool').columns\n",
        "sales_modelling_encoded_vif[bool_cols] = sales_modelling_encoded_vif[bool_cols].astype(int)\n",
        "\n",
        "# Calculate the VIF for each variable.\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data[\"feature\"] = sales_modelling_encoded_vif.columns\n",
        "vif_data[\"VIF\"] = [\n",
        "    variance_inflation_factor(sales_modelling_encoded_vif.values, i)\n",
        "    for i in range(len(sales_modelling_encoded_vif.columns))\n",
        "]\n",
        "\n",
        "vif_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3d5334c9"
      },
      "outputs": [],
      "source": [
        "sales_model_corr_vif = sales_modelling_encoded[[\n",
        "    'latitude', 'longitude', 'sold_units', 'month_sin', 'month_cos',\n",
        "    'day_of_week_sin', 'day_of_week_cos', 'day_of_year_sin', 'day_of_year_cos',\n",
        "    'season_Spring', 'season_Summer', 'season_Autumn',\n",
        "    'max_temp', 'precip', 'wind_dir', 'wind_speed', 'is_holiday', 'is_sport',\n",
        "    'is_sport1'\n",
        "]]\n",
        "\n",
        "# Check high-level correlation between variables.\n",
        "corr_matrix = sales_model_corr_vif.corr(numeric_only=True)\n",
        "\n",
        "# Create the heatmap for correlation\n",
        "plt.figure(figsize=(16, 16))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ca4OlFQwa4al"
      },
      "source": [
        "While colinearity is not as much of an issue for more complex models, looking at the VIF demonstrates well again the complex nature of these relationships. It may be interesting to establish the linear relationship between sold units and temperature in order to establish possibly 'lost sales' but is unlikely to be worth any further investigation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7af98008"
      },
      "outputs": [],
      "source": [
        "# Refine final DataFrame for modelling.\n",
        "sales_model_final = sales_modelling_encoded[[\n",
        "    'province',\n",
        "    'town/city',\n",
        "    'latitude',\n",
        "    'longitude',\n",
        "    'company_code',\n",
        "    'sales_location',\n",
        "    'sold_units',\n",
        "    'date',\n",
        "    'year',\n",
        "    'month',\n",
        "    'month_sin',\n",
        "    'month_cos',\n",
        "    'day',\n",
        "    'day_of_week',\n",
        "    'day_of_week_sin',\n",
        "    'day_of_week_cos',\n",
        "    'day_of_year',\n",
        "    'day_of_year_sin',\n",
        "    'day_of_year_cos',\n",
        "    'season_Spring',\n",
        "    'season_Summer',\n",
        "    'season_Autumn',\n",
        "    'avg_temp',\n",
        "    'min_temp',\n",
        "    'max_temp',\n",
        "    'ref_tavg',\n",
        "    'ref_tmin',\n",
        "    'ref_tmax',\n",
        "    'precip',\n",
        "    'wind_dir',\n",
        "    'wind_speed',\n",
        "    'avg_pressure',\n",
        "    'is_holiday',\n",
        "    'is_sport',\n",
        "    'is_sport1'\n",
        "]]\n",
        "\n",
        "sales_model_final.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "436e4565"
      },
      "outputs": [],
      "source": [
        "# Save the final modelling DataFrame.\n",
        "sales_model_final.to_csv('/content/drive/MyDrive/Employer Project/Datasets/Production Data/filtered_sales_temp_hols_fmatches/sales_model_final.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "057ec8fd"
      },
      "source": [
        "# 🔹 HistGradientBoostingRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "on7BK4NigO0s"
      },
      "source": [
        "The following features were selected because the model inherently captures time-based patterns, such as weekly, monthly, and seasonal trends, through inputs like day of year and sine/cosine transformations of weekday and month.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0249033b"
      },
      "outputs": [],
      "source": [
        "df_hgb = sales_model_final.copy()[[\n",
        "    'sales_location',\n",
        "    'sold_units',\n",
        "    'date',\n",
        "    'day_of_week_cos',\n",
        "    'day_of_week_sin',\n",
        "    'year',\n",
        "    'month',\n",
        "    'month_sin',\n",
        "    'month_cos',\n",
        "    'day',\n",
        "    'day_of_week',\n",
        "    'day_of_year',\n",
        "    'avg_temp',\n",
        "    'min_temp',\n",
        "    'max_temp',\n",
        "    'precip',\n",
        "    'wind_dir',\n",
        "    'wind_speed',\n",
        "    'avg_pressure',\n",
        "    'is_holiday',\n",
        "    'town/city',\n",
        "    'company_code',\n",
        "    'province'\n",
        "]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04Tkam81hYTR"
      },
      "outputs": [],
      "source": [
        "# Create named df & sort by date to prepare for timeseriessplit\n",
        "df_hgb['date'] = pd.to_datetime(df_hgb['date'])\n",
        "df_hgb = df_hgb.sort_values(by='date')\n",
        "\n",
        "df_hgb['week'] = df_hgb['date'].dt.isocalendar().week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "682c2ff2"
      },
      "outputs": [],
      "source": [
        "df_hgb.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lk0_XEKRoPcJ"
      },
      "source": [
        "Since many sales locations sold zero units on over half the days in 2021 and 2022, we aggregated sold units at the town/city level to help the model recognize patterns and improve predictability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2567f2d"
      },
      "outputs": [],
      "source": [
        "# 1. Aggregate data by 'date' and 'town/city'\n",
        "df_hgb['date'] = pd.to_datetime(df_hgb['date'])\n",
        "\n",
        "df_agg = df_hgb.groupby(['date', 'town/city']).agg({\n",
        "    'sold_units': 'sum',\n",
        "    'avg_temp': 'mean',\n",
        "    'min_temp': 'mean',\n",
        "    'max_temp': 'mean',\n",
        "    'precip': 'mean',\n",
        "    'wind_dir': 'mean',\n",
        "    'wind_speed': 'mean',\n",
        "    'avg_pressure': 'mean',\n",
        "    'is_holiday': 'max',  # assuming binary\n",
        "    'province': 'first',\n",
        "    'company_code': 'first',\n",
        "    'week': 'first',\n",
        "    'day_of_week_cos': 'mean',\n",
        "    'day_of_week_sin': 'mean',\n",
        "    'year': 'first',\n",
        "    'month': 'first',\n",
        "    'month_sin': 'mean',\n",
        "    'month_cos': 'mean',\n",
        "    'day': 'first',\n",
        "    'day_of_week': 'first',\n",
        "    'day_of_year': 'first'\n",
        "}).reset_index()\n",
        "\n",
        "# 2. Encode categorical columns ('town/city', 'province', 'company_code')\n",
        "label_encoders = {}\n",
        "\n",
        "for col in ['town/city', 'province', 'company_code']:\n",
        "    le = LabelEncoder()\n",
        "    df_agg[col] = le.fit_transform(df_agg[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# 3. Sort by date and set date index for timeseries splitting\n",
        "df_agg = df_agg.sort_values(by='date')\n",
        "df_agg.set_index('date', inplace=True)\n",
        "\n",
        "# 4. Prepare features and target for modeling\n",
        "X = df_agg.drop(columns=['sold_units'])\n",
        "y = df_agg['sold_units']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gCC0jnBj1mw"
      },
      "outputs": [],
      "source": [
        "# Evaluate time series splits\n",
        "evaluate_time_series_split_masked(HistGradientBoostingRegressor(), df_agg, y_col='sold_units')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ed65f18"
      },
      "source": [
        "We decided to proceed with 9 splits, as it offers the lowest mean MSE (333.99) among all configurations, indicating the best overall predictive accuracy. Additionally, it maintains a relatively low standard deviation (214.68), suggesting consistent performance across folds. This setup strikes a strong balance between evaluation stability and model performance without excessively fragmenting the time series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1756QDaaj1cY"
      },
      "outputs": [],
      "source": [
        "# 5. Create TimeSeriesSplit (assuming you have your custom function)\n",
        "splits = get_timeseries_splits_with_masking(df_agg, 'sold_units', n_splits=9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrW6zTGEppyM"
      },
      "source": [
        "The hyperparameters were selected as follows to ensure optimal model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TixIc3QjgIhU"
      },
      "outputs": [],
      "source": [
        "# Choose hyperparameters\n",
        "params = {\n",
        "    'max_leaf_nodes': 40,\n",
        "    'min_samples_leaf': 25,\n",
        "    'l2_regularization': 1.0,\n",
        "    'random_state': seed\n",
        "}\n",
        "\n",
        "# Tune model with chosen parameters.\n",
        "final_model, fold_rmse, cv_rmse = tune_hgb_model_with_masking(\n",
        "    df=df_agg,\n",
        "    y='sold_units',\n",
        "    params=params,\n",
        "    n_splits=9,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLajPhp6gQk2"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store performance metrics for each fold\n",
        "best_model, fold_results, avg_results, X_test, y_test = evaluate_hgb_performance_with_best_model_masked(\n",
        "    df_agg, 'sold_units', n_splits=9, best_model=final_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "398b89e0"
      },
      "source": [
        "As a next step, we are removing features with negative permutation importance, as these features reduce model performance by introducing noise rather than contributing useful predictive information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f5e7d8c"
      },
      "outputs": [],
      "source": [
        "# Calculate permutation importance with explicit scoring (e.g., neg_mean_squared_error for regression)\n",
        "result = permutation_importance(\n",
        "    best_model, X_test, y_test,\n",
        "    n_repeats=10, random_state=seed,\n",
        "    scoring='neg_mean_squared_error'\n",
        ")\n",
        "\n",
        "# Convert to pandas Series\n",
        "importance_scores = pd.Series(result.importances_mean, index=X_test.columns)\n",
        "\n",
        "# Print all feature importances sorted\n",
        "print(\"Permutation feature importances (mean decrease in score):\")\n",
        "print(importance_scores.sort_values(ascending=False))\n",
        "\n",
        "# Identify negative features (features that *improve* the score when permuted)\n",
        "negative_features = importance_scores[importance_scores < 0].index.tolist()\n",
        "\n",
        "print(\"\\nNegative features based on permutation importance (possibly harming the model):\")\n",
        "print(negative_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7586c7f7"
      },
      "outputs": [],
      "source": [
        "# Removing the negative features\n",
        "df_agg = df_agg.drop(columns=negative_features)\n",
        "df_agg.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Md6-ke6qhtC"
      },
      "source": [
        "As a next step, we created a file containing information about local events, such as sports events, which are somewhat predictable and typically recur annually around the same weeks or months. While it's not feasible to forecast the exact dates of these events in future years, we generated weekly and monthly event flags based on historical patterns. These features can be used in predictions to help the model account for the impact of recurring events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUFkGhit17Fm"
      },
      "outputs": [],
      "source": [
        "filepath_fixed_events_2 = '/content/drive/MyDrive/Employer Project/Datasets/Production Data/filtered_sales_temp_hols_fmatches/fixed_events_2.csv'\n",
        "\n",
        "# Load the dataset.\n",
        "events = pd.read_csv(filepath_fixed_events_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79q4PiHz16tp"
      },
      "outputs": [],
      "source": [
        "events.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MR99imVf16qB"
      },
      "outputs": [],
      "source": [
        "# Ensure date columns are datetime\n",
        "events['start_date'] = pd.to_datetime(events['start_date'])\n",
        "events['end_date'] = pd.to_datetime(events['end_date'])\n",
        "\n",
        "# STEP 1: Add 'event_day_flag' for specific event days\n",
        "event_dates = set()\n",
        "for _, row in events.iterrows():\n",
        "    dates = pd.date_range(row['start_date'], row['end_date'])\n",
        "    event_dates.update(dates)\n",
        "\n",
        "# Add binary flag for each date\n",
        "df_agg = df_agg.reset_index()\n",
        "df_agg['event_day_flag'] = df_agg['date'].isin(event_dates).astype(int)\n",
        "\n",
        "# STEP 2: Prepare weekly/monthly event counts from events\n",
        "weekly_event_counts = events.groupby('weekly_flag')['weekly_event_count'].sum().reset_index()\n",
        "monthly_event_counts = events.groupby('monthly_flag')['monthly_event_count'].sum().reset_index()\n",
        "\n",
        "# STEP 3: Merge with main df using 'week' and 'month'\n",
        "df_agg['week'] = df_agg['date'].dt.strftime('%Y-W%U')  # Match weekly_flag format\n",
        "df_agg['month'] = df_agg['date'].dt.strftime('%Y-%m')  # Match monthly_flag format\n",
        "\n",
        "df_agg = df_agg.merge(weekly_event_counts, how='left', left_on='week', right_on='weekly_flag')\n",
        "df_agg = df_agg.merge(monthly_event_counts, how='left', left_on='month', right_on='monthly_flag')\n",
        "\n",
        "# Fill missing event counts with 0 (i.e., no events that week/month)\n",
        "df_agg['weekly_event_count'] = df_agg['weekly_event_count'].fillna(0)\n",
        "df_agg['monthly_event_count'] = df_agg['monthly_event_count'].fillna(0)\n",
        "\n",
        "# Final clean-up\n",
        "df_agg = df_agg.drop(columns=['weekly_flag', 'monthly_flag'])\n",
        "\n",
        "# Restore index\n",
        "df_agg = df_agg.sort_values('date')\n",
        "df_agg.set_index('date', inplace=True)\n",
        "\n",
        "# After merging weekly/monthly counts\n",
        "df_agg.drop(columns=['week', 'month'], inplace=True, errors='ignore')\n",
        "\n",
        "df_agg = df_agg.drop(columns=['event_day_flag'])\n",
        "\n",
        "X = df_agg.drop(columns=['sold_units'])\n",
        "y = df_agg['sold_units']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2bSATddf-6j"
      },
      "outputs": [],
      "source": [
        "# Create TimeSeriesSplit.\n",
        "splits = get_timeseries_splits_with_masking(df_agg, 'sold_units', n_splits=9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXhLirbH16i-"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "    'max_leaf_nodes': 40,\n",
        "    'min_samples_leaf': 25,\n",
        "    'l2_regularization': 1.0,\n",
        "    'random_state': seed\n",
        "}\n",
        "\n",
        "\n",
        "final_model, fold_rmse, cv_rmse = tune_hgb_model_with_masking(\n",
        "    df=df_agg,\n",
        "    y='sold_units',\n",
        "    params=params,\n",
        "    n_splits=9,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAt4DuEP16e5"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store performance metrics for each fold\n",
        "best_model, fold_results, avg_results, X_test, y_test = evaluate_hgb_performance_with_best_model_masked(\n",
        "    df_agg, 'sold_units', n_splits=9, best_model=final_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMvMakFx16L9"
      },
      "outputs": [],
      "source": [
        "# Check for overfitting\n",
        "check_overfitting_on_last_fold_masked(df_agg, y_col='sold_units', n_splits=9, model=best_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xakZ05O15Uo"
      },
      "outputs": [],
      "source": [
        "unique_dates = df_agg.index.unique().sort_values()\n",
        "tscv = TimeSeriesSplit(n_splits=9)\n",
        "\n",
        "for train_idx, test_idx in tscv.split(unique_dates):\n",
        "    pass  # This iterates through all, so at the end you'll have the last indices\n",
        "\n",
        "train_dates = set(unique_dates[train_idx])\n",
        "test_dates = set(unique_dates[test_idx])\n",
        "\n",
        "# Use df_agg's index to create masks, not hgb_model's\n",
        "train_mask = df_agg.index.map(train_dates.__contains__)\n",
        "test_mask = df_agg.index.map(test_dates.__contains__)\n",
        "\n",
        "X = df_agg.drop(columns=['sold_units'])\n",
        "y = df_agg['sold_units']\n",
        "\n",
        "X_train, y_train = X[train_mask], y[train_mask]\n",
        "X_test, y_test = X[test_mask], y[test_mask]\n",
        "\n",
        "# Predict on training fold only\n",
        "y_train_pred = best_model.predict(X_train)\n",
        "train_residuals = y_train - y_train_pred\n",
        "\n",
        "# 1. Residuals vs. Predicted (Train only)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.scatterplot(x=y_train_pred, y=train_residuals, alpha=0.6)\n",
        "plt.axhline(0, color='red', linestyle='--')\n",
        "plt.title(\"Residuals vs. Predicted Values (Train Only)\")\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Histogram of Residuals (Train only)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.histplot(train_residuals, bins=50, kde=True)\n",
        "plt.axvline(0, color='red', linestyle='--')\n",
        "plt.title(\"Histogram of Residuals (Train Only)\")\n",
        "plt.xlabel(\"Residual\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKn2oXWQ2jQI"
      },
      "outputs": [],
      "source": [
        "explainer = shap.Explainer(best_model)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "shap.plots.beeswarm(shap_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyzuUNLj1woV"
      },
      "outputs": [],
      "source": [
        "df_hgb = df_agg.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70a152cd"
      },
      "source": [
        "### Observations\n",
        "This final model demonstrates strong performance across the cross-validation folds. The average Mean Squared Error (MSE) is 287.50, with a Root Mean Squared Error (RMSE) of 16.05 and a Mean Absolute Error (MAE) of 6.38. The model explains about 66% of the variance in the target variable, as shown by the average R². A high Mean Absolute Percentage Error (MAPE) of 169.66% reflects considerable variability or scale issues in the target.\n",
        "\n",
        "Overfitting is evident in the final fold, where training R² is 0.8917 and test R² drops to 0.6218, showing the model fits training data better than new data. Despite this, no further improvements will be made.\n",
        "\n",
        "Residuals are mostly well distributed, with a slight leftward skew suggesting minor underprediction in some cases. Overall, no strong bias or heteroscedasticity is observed.\n",
        "\n",
        "Incorporating more historical data could help reduce overfitting and lower the high MAPE by improving the model's generalization and handling of target variability. However, this model is considered final for the current project, and we will now proceed to build confidence intervals using Monte Carlo simulation to further assess its predictive uncertainty."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7fb2c65"
      },
      "source": [
        "# 🔹 Prepare Features for Forecasting Period"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b78cf067"
      },
      "source": [
        "## Check Model Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ae1e84b"
      },
      "outputs": [],
      "source": [
        "feature_names = best_model.feature_names_in_\n",
        "\n",
        "feature_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "783a0bf0"
      },
      "source": [
        "## Create Temporal Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a74f5e01"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame with dates.\n",
        "forecast_df_calendar = generate_static_calendar_features(2023)\n",
        "\n",
        "forecast_df_calendar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44de78d4"
      },
      "source": [
        "## Repanel Forecast Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2UMV38lDVgF"
      },
      "outputs": [],
      "source": [
        "X_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05032db4"
      },
      "outputs": [],
      "source": [
        "# Create forecast panel with location info\n",
        "one_hot_cols = ['town/city', 'company_code']\n",
        "\n",
        "# Remove station_code from one_hot_columns if present to avoid duplication\n",
        "if 'town/city' == 'town/city' and 'town/city' in one_hot_cols:\n",
        "    one_hot_cols.remove('town/city')\n",
        "\n",
        "forecast_df_panel = create_forecast_panel(\n",
        "    X_test,\n",
        "    forecast_df_calendar,\n",
        "    station_code='town/city',\n",
        "    one_hot_columns=one_hot_cols\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5a2Bwxl4kx1"
      },
      "outputs": [],
      "source": [
        "for i, col in enumerate(forecast_df_panel.columns):\n",
        "    print(i, repr(col))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed0e12bb"
      },
      "source": [
        "## Add Holiday Flag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39583db3"
      },
      "source": [
        "### Load Holidays Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6e3f7183"
      },
      "outputs": [],
      "source": [
        "# Load 2023 holidays data\n",
        "filepath_city_holidays_2023 = '/content/drive/MyDrive/Employer Project/Datasets/Production Data/filtered_sales_temp_hols_fmatches/city_holidays_2023.csv'\n",
        "\n",
        "# Load the dataset.\n",
        "city_holidays_2023 = pd.read_csv(filepath_city_holidays_2023)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82f35e3f"
      },
      "source": [
        "### Map Holiday Flag to Forecast Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24ad786d"
      },
      "outputs": [],
      "source": [
        "# Convert date to datetime ensruing the day first read\n",
        "city_holidays_2023['date'] = pd.to_datetime(city_holidays_2023['date'], dayfirst=True)\n",
        "\n",
        "# Create set of holiday dates\n",
        "hols_2023 = set(city_holidays_2023['date'])\n",
        "\n",
        "# Add is_holiday flag\n",
        "forecast_df_panel['is_holiday'] = forecast_df_panel['date'].isin(hols_2023)\n",
        "\n",
        "forecast_df_panel.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AhzHHMlsQ_q"
      },
      "source": [
        "##Add event weekly and monthly flags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6n35c2YsZgl"
      },
      "outputs": [],
      "source": [
        "events.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zLO4nbLs1PZ"
      },
      "outputs": [],
      "source": [
        "# 1. Extract week and month from forecast_df_panel (for 2023)\n",
        "forecast_df_panel['week'] = forecast_df_panel['date'].dt.isocalendar().week\n",
        "\n",
        "# 2. Create average event counts per week and month from 2021–2022\n",
        "\n",
        "# Make sure event columns are numeric\n",
        "events['weekly_event_count'] = pd.to_numeric(events['weekly_event_count'], errors='coerce')\n",
        "events['monthly_event_count'] = pd.to_numeric(events['monthly_event_count'], errors='coerce')\n",
        "\n",
        "# Extract week and month from the event calendar\n",
        "events['week'] = pd.to_datetime(events['start_date']).dt.isocalendar().week\n",
        "events['month'] = pd.to_datetime(events['start_date']).dt.month\n",
        "\n",
        "# Group by week and month to calculate average event counts (across 2021–2022)\n",
        "weekly_avg = events.groupby('week')['weekly_event_count'].mean().reset_index()\n",
        "monthly_avg = events.groupby('month')['monthly_event_count'].mean().reset_index()\n",
        "\n",
        "# 3. Merge the averages into 2023 forecast panel\n",
        "forecast_df_panel = forecast_df_panel.merge(weekly_avg, on='week', how='left')\n",
        "forecast_df_panel = forecast_df_panel.merge(monthly_avg, on='month', how='left')\n",
        "\n",
        "# 4. Rename the merged columns to match the model's expected feature names\n",
        "forecast_df_panel.rename(columns={\n",
        "    'weekly_event_count': 'weekly_event_count',\n",
        "    'monthly_event_count': 'monthly_event_count'\n",
        "}, inplace=True)\n",
        "\n",
        "# Optional: Fill any remaining NaNs with 0\n",
        "forecast_df_panel['weekly_event_count'] = forecast_df_panel['weekly_event_count'].fillna(0)\n",
        "forecast_df_panel['monthly_event_count'] = forecast_df_panel['monthly_event_count'].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFmWU-jNtNDh"
      },
      "outputs": [],
      "source": [
        "forecast_df_panel.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06f393bb"
      },
      "source": [
        "## Align Final Dataframe with Model Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12adf0c3"
      },
      "outputs": [],
      "source": [
        "# Set index to datetime.\n",
        "forecast_df_panel = forecast_df_panel.set_index(pd.to_datetime(forecast_df_panel['date']))\n",
        "\n",
        "required_columns = ['avg_temp', 'min_temp', 'max_temp', 'precip', 'wind_dir', 'wind_speed', 'avg_pressure']\n",
        "\n",
        "for col in required_columns:\n",
        "    if col not in forecast_df_panel.columns:\n",
        "        forecast_df_panel[col] = np.nan\n",
        "\n",
        "final_features = forecast_df_panel[best_model.feature_names_in_]\n",
        "final_features.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7723e693"
      },
      "outputs": [],
      "source": [
        "# Double check feature names aligned\n",
        "X_test.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZJss4D94lyM"
      },
      "source": [
        "# 🔹 Monte Carlo Simulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8_VWAohoHTw"
      },
      "source": [
        "## Forecast Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DK997hP00gy"
      },
      "source": [
        "Our Gradient Boost model uses past data to understand how variables such as temperature, time of year, and location influence product demand (sold units). Then, to understand how demand might look under different future weather scenarios, we simulated possible temperature outcomes based on 30 years of historical weather data from [Meteostat](https://meteostat.net/en/) . By running our Boost model over these simulations, we are able to generate ranges of likely demand, giving us not just a single forecast — but a probability distribution with confidence intervals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e9ea270"
      },
      "source": [
        "## Retrieve Climate Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64d7df0c"
      },
      "outputs": [],
      "source": [
        "# Retrieve Climate Data\n",
        "temp_stats_hist = get_daily_variability()\n",
        "\n",
        "# Check the climate data.\n",
        "temp_stats_hist.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eff420a8"
      },
      "source": [
        "## Create Climate Scenarios"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96be3885"
      },
      "outputs": [],
      "source": [
        "# Run custom function to create dataframe for all temperature varibles for year 2023\n",
        "future_climate_scenarios = generate_future_scenarios(temp_stats_hist, year=2023)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97e10517"
      },
      "source": [
        "## Perform Monte Carlo Simulation and Forecast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d494ddd2"
      },
      "outputs": [],
      "source": [
        "# Create dataframe with columns for the simulation results\n",
        "predictions_2023 = run_simulation(future_climate_scenarios, forecast_df_panel, best_model, sample_uniform_monte, n_simulations=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c0a3ab6"
      },
      "outputs": [],
      "source": [
        "predictions_2023.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9330a8c0"
      },
      "outputs": [],
      "source": [
        "# Store 2023 predictions output as CSV\n",
        "predictions_2023.to_csv('/content/drive/MyDrive/Employer Project/Datasets/Production Data/filtered_sales_temp_hols_fmatches/predictions_2023.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23653b6d"
      },
      "source": [
        "## Visualise Model Forecast for Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5xBe9DqgxAG"
      },
      "source": [
        "In this last part, we are exploring different visualisations of our model and the predictions to simplify the output for a stakeholder that we assume is not as familiar with machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe06676f"
      },
      "outputs": [],
      "source": [
        "predictions_2023.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd6196dd"
      },
      "outputs": [],
      "source": [
        "# Convert 'date' column to datetime if not already\n",
        "sales_model_final['date'] = pd.to_datetime(sales_model_final['date'])\n",
        "\n",
        "# Aggregate sold_units monthly by year and month\n",
        "monthly_sales = sales_model_final.groupby(['year', 'month']).agg({\n",
        "    'sold_units': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# Create a datetime column for the first day of the month\n",
        "monthly_sales['month_start'] = pd.to_datetime(monthly_sales[['year', 'month']].assign(day=1))\n",
        "\n",
        "# Add abbreviated month name\n",
        "monthly_sales['month_name'] = monthly_sales['month'].apply(lambda x: calendar.month_abbr[x])\n",
        "\n",
        "print(monthly_sales.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9befe1d2"
      },
      "outputs": [],
      "source": [
        "# Ensure date is datetime\n",
        "predictions_2023['date'] = pd.to_datetime(predictions_2023['date'])\n",
        "\n",
        "# Add month column\n",
        "predictions_2023['month'] = predictions_2023['date'].dt.to_period('M').dt.to_timestamp()\n",
        "\n",
        "# Aggregate total sold units per month for each scenario including 95% bounds\n",
        "monthly = predictions_2023.groupby('month').agg({\n",
        "    'mean': 'sum',       # Average scenario\n",
        "    'low_80': 'sum',     # Pessimistic (80% lower bound)\n",
        "    'up_80': 'sum',      # Optimistic (80% upper bound)\n",
        "    'low_95': 'sum',     # Pessimistic (95% lower bound)\n",
        "    'up_95': 'sum',      # Optimistic (95% upper bound)\n",
        "}).reset_index()\n",
        "\n",
        "print(predictions_2023.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8e82d79"
      },
      "outputs": [],
      "source": [
        "# Load the dataset.\n",
        "predict_23 = predictions_2023.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9c84a73"
      },
      "outputs": [],
      "source": [
        "# Create visual for 2023 forecast and confidence bands only\n",
        "# Create 'year_month' as datetime for proper sorting\n",
        "predict_23['year_month'] = pd.to_datetime(predict_23['date'].dt.to_period('M').astype(str))\n",
        "\n",
        "# Create short month name for display (e.g., \"Jan\", \"Feb\", ...)\n",
        "predict_23['month_label'] = predict_23['date'].dt.strftime('%b')\n",
        "\n",
        "# Define aggregation functions\n",
        "agg_functions = {\n",
        "    'mean': 'sum',\n",
        "    'low_80': 'sum',\n",
        "    'up_80': 'sum',\n",
        "    'low_95': 'sum',\n",
        "    'up_95': 'sum',\n",
        "    'forecasted_max_temp': 'mean',\n",
        "    'prob_exceed_28': 'mean',\n",
        "    'conditional_mean_temp': 'mean'\n",
        "}\n",
        "\n",
        "# Group by 'year_month' and aggregate\n",
        "monthly_forecast = predict_23.groupby('year_month').agg(agg_functions).reset_index()\n",
        "\n",
        "# Add month name back in for display (now matched to ordered months)\n",
        "monthly_forecast['month_label'] = monthly_forecast['year_month'].dt.strftime('%b')\n",
        "\n",
        "# Drop any rows with missing values in CI columns\n",
        "monthly_forecast = monthly_forecast.dropna(subset=['low_80', 'up_80', 'low_95', 'up_95'])\n",
        "\n",
        "# Function to format numbers in thousands with no decimals\n",
        "def format_thousands(value):\n",
        "    # Convert to integer after dividing by 1000 to remove decimal point\n",
        "    return f'{int(value // 1000)}k' if value >= 1000 else f'{int(value)}'\n",
        "\n",
        "def plot_summary_month(df, x_labels, title):\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "    # Plot mean line\n",
        "    ax.plot(x_labels, df['mean'], label='Sales Forecast')\n",
        "\n",
        "    # Plot the fill between for confidence intervals\n",
        "    ax.fill_between(x_labels, df['low_80'], df['up_80'], alpha=0.3, label='80% Confidence Interval')\n",
        "    ax.fill_between(x_labels, df['low_95'], df['up_95'], alpha=0.2, label='95% Confidence Interval')\n",
        "\n",
        "    # Add labels at the end of the confidence intervals in thousands (no decimals)\n",
        "    for i, month in enumerate(x_labels):\n",
        "        ax.text(i, df['up_80'][i], format_thousands(df['up_80'][i]), color='black', ha='center', va='bottom', fontsize=10)\n",
        "        ax.text(i, df['up_95'][i], format_thousands(df['up_95'][i]), color='black', ha='center', va='bottom', fontsize=10)\n",
        "        ax.text(i, df['mean'][i], format_thousands(df['mean'][i]), color='black', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    # Add titles and labels\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(\"Month\")\n",
        "    ax.set_ylabel(\"Units - Sales Forecast / Probability\")\n",
        "\n",
        "    # Add a custom legend for the labels (indicating units)\n",
        "    ax.legend(title= \"# = Thousand Units\", loc='upper left')\n",
        "\n",
        "    # Grid settings\n",
        "    ax.grid(axis='x', visible=True)\n",
        "    ax.grid(axis='y', visible=False)\n",
        "\n",
        "    # Set x-axis labels and rotation\n",
        "    ax.set_xticks(range(len(x_labels)))\n",
        "    ax.set_xticklabels(x_labels, rotation=45, ha='right')\n",
        "\n",
        "    # Tight layout to ensure no clipping of labels\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_summary_month(monthly_forecast, monthly_forecast['month_label'].tolist(), \"2023 Forecast with Confidence Bands\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DA5fVCXkeVrD"
      },
      "outputs": [],
      "source": [
        "# Prepare historical data for 2021 and 2022\n",
        "sales_model_final['date'] = pd.to_datetime(sales_model_final['date'])\n",
        "historical = sales_model_final[sales_model_final['year'].isin([2021, 2022])]\n",
        "historical_monthly = historical.groupby(['year', 'month']).agg({\n",
        "    'sold_units': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# Prepare predictions for 2023: mean and up_80\n",
        "predictions_2023['date'] = pd.to_datetime(predictions_2023['date'])\n",
        "monthly_2023 = predictions_2023.groupby(predictions_2023['date'].dt.month).agg({\n",
        "    'mean': 'sum',\n",
        "    'up_80': 'sum'\n",
        "}).reset_index().rename(columns={'date': 'month'})\n",
        "\n",
        "# Month abbreviations\n",
        "months_abbr = [calendar.month_abbr[m] for m in range(1, 13)]\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot bars for 2021 and 2022 side-by-side\n",
        "bar_width = 0.35\n",
        "x = np.arange(1, 13)\n",
        "\n",
        "# Filter data for bars\n",
        "data_2021 = historical_monthly[historical_monthly['year'] == 2021].sort_values('month')\n",
        "data_2022 = historical_monthly[historical_monthly['year'] == 2022].sort_values('month')\n",
        "\n",
        "# Adjusted colors\n",
        "color_2021 = '#6baed6'  # Lighter blue for 2021\n",
        "color_2022 = '#c6dbef'  # Even lighter for 2022\n",
        "\n",
        "plt.bar(x - bar_width/2, data_2021['sold_units'], width=bar_width, label='2021 Actuals', color=color_2021)\n",
        "plt.bar(x + bar_width/2, data_2022['sold_units'], width=bar_width, label='2022 Actuals', color=color_2022)\n",
        "\n",
        "# Plot shaded area between mean and up_80\n",
        "plt.fill_between(monthly_2023['month'], monthly_2023['mean'], monthly_2023['up_80'],\n",
        "                 color='red', alpha=0.15, label='2023 Range (Avg to Upper 80%)')\n",
        "\n",
        "# Plot mean (dashed black) and up_80 (solid red) lines with dots\n",
        "plt.plot(monthly_2023['month'], monthly_2023['mean'], color='black', linestyle='--', linewidth=1.5, marker='o', label='2023 Average')\n",
        "plt.plot(monthly_2023['month'], monthly_2023['up_80'], color='red', linestyle='-', linewidth=2, marker='o', label='2023 Upper 80% (recommended)')\n",
        "\n",
        "# Add data labels with different offsets in 'k' format\n",
        "mean_offset = max(monthly_2023['up_80']) * 0.01\n",
        "up80_offset = max(monthly_2023['up_80']) * 0.04\n",
        "\n",
        "for x_pos, y_val in zip(monthly_2023['month'], monthly_2023['mean']):\n",
        "    plt.text(x_pos, y_val + mean_offset, f\"{int(y_val/1000)}k\", ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "for x_pos, y_val in zip(monthly_2023['month'], monthly_2023['up_80']):\n",
        "    plt.text(x_pos, y_val + up80_offset, f\"{int(y_val/1000)}k\", ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "# Y-axis format: e.g., 10'000\n",
        "def format_thousands(x, pos):\n",
        "    return f\"{int(x):,}\".replace(\",\", \"'\")\n",
        "\n",
        "plt.gca().yaxis.set_major_formatter(FuncFormatter(format_thousands))\n",
        "\n",
        "# X-axis ticks and labels\n",
        "plt.xticks(ticks=x, labels=months_abbr)\n",
        "plt.ylim(0, 40000)  # <- Adjusted from 45000 to 42000\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Total Units')\n",
        "plt.title('Monthly Sales: 2021–2022 Actuals & 2023 Predictions')\n",
        "plt.legend()\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n_9snGDfMF_"
      },
      "source": [
        "#🔹 __Revenue Optimisation__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j0_TwzQsXH1"
      },
      "source": [
        ">\n",
        "### __Data Sources and Preparation__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX3V2k5PlE_L"
      },
      "outputs": [],
      "source": [
        "# Locate the dataset in Google Drive.\n",
        "filepath_sales_model_final = '/content/drive/MyDrive/Employer Project/Datasets/Production Data/filtered_sales_temp_hols_fmatches/sales_model_final.csv'\n",
        "\n",
        "# Load the dataset.\n",
        "sales_model_final = pd.read_csv(filepath_sales_model_final)\n",
        "\n",
        "# Sense check the DataFrame.\n",
        "sales_model_final.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoY3xeStsddo"
      },
      "source": [
        ">\n",
        "### __Setting Model Parameters/Calculations__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iqJSlYn2sdvt"
      },
      "outputs": [],
      "source": [
        "# Visualise max temp and sales to derive temperature thresholds.\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Set bin width for temperature (2-degree bins).\n",
        "bin_width = 2\n",
        "bins = range(0, 44, bin_width)\n",
        "\n",
        "# Calculate the average units sold for each temperature bin.\n",
        "avg_units_per_bin = sales_model_final.groupby(pd.cut(sales_model_final['max_temp'], bins=bins))['sold_units'].mean()\n",
        "\n",
        "# Plotthe average units sold.\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(avg_units_per_bin.index.astype(str), avg_units_per_bin.values, color='blue', alpha=0.7, edgecolor='black')\n",
        "\n",
        "# Add labels and title.\n",
        "plt.title('Average Units Sold by Max Temperature')\n",
        "plt.xlabel('Max Temperature (°C)')\n",
        "plt.ylabel('Average Units Sold')\n",
        "\n",
        "# Rotate x-axis labels for better readability.\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Show the plot.\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxKHGzYkslIQ"
      },
      "source": [
        "__Observations:__ Set temperature threshold for which more units should be considered. Max temperature and previous exploration demonstrates that sales start to peak around 26-28 degrees max temp. Extreme maximum temperatures are also demonstrated as peaks around 36 to 40 (but these temperatures are not considered normal). Will set the temperature threshold to 26."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAu4C2wrsljn"
      },
      "outputs": [],
      "source": [
        "# Visualise differences in temp differences from reference normals across months.\n",
        "# Group the data by month and calculate the average temp_diff.\n",
        "sales_model_final['temp_diff'] = (sales_model_final['max_temp'] - sales_model_final['ref_tmax'])\n",
        "avg_temp_diff_per_month = sales_model_final.groupby('month')['temp_diff'].mean()\n",
        "\n",
        "# Plotting the average temperature difference per month.\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.bar(avg_temp_diff_per_month.index.astype(str), avg_temp_diff_per_month.values, color='blue', alpha=0.7, edgecolor='black')\n",
        "\n",
        "# Add labels and title.\n",
        "plt.title('Average Temperature Difference by Month')\n",
        "plt.xlabel('Month')\n",
        "plt.ylabel('Average Temperature Difference (°C)')\n",
        "\n",
        "# Customize x-axis to display months in order.\n",
        "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
        "plt.xticks(ticks=range(12), labels=months, rotation=45)\n",
        "\n",
        "# Set y-axis to start from the minimum value in the data.\n",
        "min_temp_diff = avg_temp_diff_per_month.min()\n",
        "max_temp_diff = avg_temp_diff_per_month.max()\n",
        "padding = 2\n",
        "plt.ylim(min_temp_diff - padding, max_temp_diff + padding)\n",
        "\n",
        "# Add a horizontal line at y=0.\n",
        "plt.axhline(0, color='black', linewidth=1.2, linestyle='-')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XO605saRsuRc"
      },
      "source": [
        ">\n",
        "### __Requirements From the Model__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_f68t4EszFJ"
      },
      "source": [
        "#### __Load the Predictions for 2023__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zis9a6ZTs1Hu"
      },
      "outputs": [],
      "source": [
        "# Locate the final predictions file for 2023.\n",
        "filepath_2023_predictions = '/content/drive/MyDrive/Employer Project/Datasets/Production Data/filtered_sales_temp_hols_fmatches/predictions_2023.csv'\n",
        "\n",
        "# Load the dataset.\n",
        "predictions_temps_df = pd.read_csv(filepath_2023_predictions)\n",
        "\n",
        "# Sense check the dataset.\n",
        "predictions_temps_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAz2gy7Ms3nV"
      },
      "outputs": [],
      "source": [
        "# Sense check head of DataFrame\n",
        "print(predictions_temps_df[['mean', 'low_80', 'up_80', 'low_95', 'up_95', 'forecasted_max_temp', 'prob_exceed_28', 'conditional_mean_temp']].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx7fbZfIs7j3"
      },
      "source": [
        "#### __Load the Events Data for 2023__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9IaTozEs8Ja"
      },
      "outputs": [],
      "source": [
        "# Locate the final events file for 2023.\n",
        "filepath_2023_events = '/content/drive/MyDrive/Employer Project/Datasets/Production Data/filtered_sales_temp_hols_fmatches/fixed_events_2023.csv'\n",
        "\n",
        "# Load the dataset.\n",
        "events_2023_df = pd.read_csv(filepath_2023_events)\n",
        "\n",
        "# Sense check the dataset.\n",
        "events_2023_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlpNoYDztAkY"
      },
      "outputs": [],
      "source": [
        "# Update date columns to datetime.\n",
        "events_2023_df[['start_date', 'end_date']] = events_2023_df[['start_date', 'end_date']].apply(pd.to_datetime)\n",
        "\n",
        "# Sense-check the DataFrame.\n",
        "events_2023_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhwQ4ZqUtEbS"
      },
      "outputs": [],
      "source": [
        "# Apply a boolean is_event_day flag to the predictions DataFrame.\n",
        "predictions_temps_df['is_event_day'] = predictions_temps_df['date'].apply(\n",
        "    lambda x: ((events_2023_df['start_date'] <= x) & (events_2023_df['end_date'] >= x)).any()\n",
        ")\n",
        "\n",
        "# Sense-check DataFrame.\n",
        "predictions_temps_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3nOQO8htGdM"
      },
      "outputs": [],
      "source": [
        "# Set index to datetime.\n",
        "predictions_temps_df['date'] = pd.to_datetime(predictions_temps_df['date'])\n",
        "predictions_temps_df.set_index('date', inplace=True)\n",
        "\n",
        "# Sense check DataFrame.\n",
        "predictions_temps_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkCKIBdbtHeW"
      },
      "source": [
        ">\n",
        "### __Prepare Dataframe For Optimisation__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPz6WpkltJYP"
      },
      "source": [
        "#### __Get Daily Reference Temperatures__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d63EoeMctLGX"
      },
      "outputs": [],
      "source": [
        "# Retrieve reference normals.\n",
        "normals_df, mean_deviation, std_deviation = get_daily_tmax_normals_mean_std(predictions_temps_df, station_id='08221', k_neighbors=5)\n",
        "\n",
        "# Check 365 days, sense-check temperatures.\n",
        "normals_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lgSE4U9tPtl"
      },
      "source": [
        "#### __Calculate Z-Scores for Confidence Weighting__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyMd7fZrtTWz"
      },
      "outputs": [],
      "source": [
        "# Calculate typical deviation of historical temperatures from the reference normal\n",
        "# and use z-scores to determine how anomalous a forecasted temperature is from\n",
        "# the normal and assign a confidence weighting to the forecast.\n",
        "optimised_forecast_df = prepare_forecast_with_z_scores(predictions_temps_df, normals_df, mean_deviation, std_deviation)\n",
        "\n",
        "# Sense check the dataframe.\n",
        "optimised_forecast_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmKKATWntS-s"
      },
      "source": [
        "### __Process Model Outputs__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POx3xFA6tm72"
      },
      "source": [
        "#### __Optimise 2023 Forecast__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJTqSnQHtX7h"
      },
      "outputs": [],
      "source": [
        "# Optimise model outputs based on forecasted temperature, probability of heat and\n",
        "# forecast confidence.\n",
        "optimised_forecast_final = adjust_ratios_calc_revenue(optimised_forecast_df)\n",
        "\n",
        "optimised_forecast_final.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14WfHJ2RtbVL"
      },
      "source": [
        "#### __Add 2022 and 2021 Sales Figures for Comparison__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZA1iRf5dtrFi"
      },
      "outputs": [],
      "source": [
        "# Subset original sales data for 2022 sold units.\n",
        "sold_units_2022 = sales_model_final[sales_model_final['year'].isin([2022])]\n",
        "\n",
        "# Subset 2022 data to required columns.\n",
        "sold_units_2022 = sold_units_2022[['date', 'sold_units']]\n",
        "\n",
        "# Sense-check 2022 data.\n",
        "sold_units_2022.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8Uh8WSTceZA"
      },
      "outputs": [],
      "source": [
        "# Ensure 'date' is datetime\n",
        "sold_units_2022['date'] = pd.to_datetime(sold_units_2022['date'])\n",
        "\n",
        "# Aggregate to daily totals\n",
        "sold_units_2022_daily = sold_units_2022.groupby('date')['sold_units'].sum().reset_index()\n",
        "\n",
        "# Define full December date range\n",
        "full_december = pd.date_range(start='2022-12-01', end='2022-12-31')\n",
        "\n",
        "# Get existing December dates in data\n",
        "existing_december_dates = sold_units_2022_daily[\n",
        "    (sold_units_2022_daily['date'].dt.month == 12) &\n",
        "    (sold_units_2022_daily['date'].dt.year == 2022)\n",
        "]['date']\n",
        "\n",
        "# Find missing dates\n",
        "missing_dates = full_december.difference(existing_december_dates)\n",
        "\n",
        "# Use only early December (e.g., Dec 1–15) to calculate average\n",
        "early_december = sold_units_2022_daily[\n",
        "    (sold_units_2022_daily['date'] >= '2022-12-01') &\n",
        "    (sold_units_2022_daily['date'] <= '2022-12-15')\n",
        "]\n",
        "early_avg = early_december['sold_units'].mean()\n",
        "adjusted_value = round(early_avg * 0.77)\n",
        "\n",
        "# Create missing rows\n",
        "missing_rows = pd.DataFrame({\n",
        "    'date': missing_dates,\n",
        "    'sold_units': adjusted_value\n",
        "})\n",
        "\n",
        "# Append and sort\n",
        "sold_units_2022 = pd.concat([sold_units_2022_daily, missing_rows], ignore_index=True)\n",
        "sold_units_2022 = sold_units_2022.sort_values('date').reset_index(drop=True)\n",
        "\n",
        "# Preview\n",
        "print(sold_units_2022.tail(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5PK3ri_1Dzr"
      },
      "outputs": [],
      "source": [
        "# Define premium product parameters.\n",
        "premium_ratio = 0.02\n",
        "premium_price=1.89\n",
        "premium_margin=0.4\n",
        "\n",
        "# Define standard product parameters.\n",
        "standard_ratio = 0.08\n",
        "standard_price=1.49\n",
        "standard_margin=0.2\n",
        "\n",
        "# Calculate revenue and profit for 2022 data.\n",
        "sold_units_2022['premium_revenue_2022'] = sold_units_2022['sold_units'] * premium_ratio * premium_price\n",
        "sold_units_2022['standard_revenue_2022'] = sold_units_2022['sold_units'] * standard_ratio * standard_price\n",
        "sold_units_2022['total_revenue_2022'] = sold_units_2022['premium_revenue_2022'] + sold_units_2022['standard_revenue_2022']\n",
        "sold_units_2022['expected_profit_2022'] = (\n",
        "    sold_units_2022['premium_revenue_2022'] * premium_margin +\n",
        "    sold_units_2022['standard_revenue_2022'] * standard_margin\n",
        ")\n",
        "\n",
        "# Sense-check 2022 data.\n",
        "sold_units_2022.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFC50hicCSu1"
      },
      "outputs": [],
      "source": [
        "# Subset original sales data for 2021 sold units.\n",
        "sold_units_2021 = sales_model_final[sales_model_final['year'].isin([2021])]\n",
        "\n",
        "# Subset 2021 data to required columns.\n",
        "sold_units_2021 = sold_units_2021[['date', 'sold_units']]\n",
        "\n",
        "# Calculate revenue and profit for 2021 data.\n",
        "sold_units_2021['premium_revenue_2021'] = sold_units_2021['sold_units'] * premium_ratio * premium_price\n",
        "sold_units_2021['standard_revenue_2021'] = sold_units_2021['sold_units'] * standard_ratio * standard_price\n",
        "sold_units_2021['total_revenue_2021'] = sold_units_2021['premium_revenue_2021'] + sold_units_2021['standard_revenue_2021']\n",
        "sold_units_2021['expected_profit_2021'] = (\n",
        "    sold_units_2021['premium_revenue_2021'] * premium_margin +\n",
        "    sold_units_2021['standard_revenue_2021'] * standard_margin\n",
        ")\n",
        "\n",
        "# Sense-check 2021 data.\n",
        "sold_units_2021.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVF4vsUIcn5A"
      },
      "outputs": [],
      "source": [
        "# Prepare 2022 units for export.\n",
        "# Convert 'date' to datetime.\n",
        "sold_units_2022['date'] = pd.to_datetime(sold_units_2022['date'])\n",
        "\n",
        "# Create 'month_start_date' - first day of the month for each date.\n",
        "sold_units_2022['month_start_date'] = sold_units_2022['date'].values.astype('datetime64[M]')\n",
        "\n",
        "# Aggregate by month_start_date (monthly granularity)\n",
        "sold_units_2022_monthly = sold_units_2022.groupby('month_start_date').agg({\n",
        "    'sold_units': 'sum',\n",
        "    'premium_revenue_2022': 'sum',\n",
        "    'standard_revenue_2022': 'sum',\n",
        "    'total_revenue_2022': 'sum',\n",
        "    'expected_profit_2022': 'sum'\n",
        "}).reset_index()\n",
        "\n",
        "# Add 'year' and 'month' columns.\n",
        "sold_units_2022_monthly['year'] = sold_units_2022_monthly['month_start_date'].dt.year\n",
        "sold_units_2022_monthly['month'] = sold_units_2022_monthly['month_start_date'].dt.month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sESEZe1d4JX3"
      },
      "outputs": [],
      "source": [
        "# View 2022 data sample.\n",
        "sold_units_2022_monthly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqmVfa65trva"
      },
      "source": [
        "#### __Compare 2021 and 2022 Unoptimised Sales Figures to 2023 Optimised Sales Figures__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtOZFTyntzTJ"
      },
      "outputs": [],
      "source": [
        "# Calculate 2023 totals.\n",
        "total_sales_2023 = optimised_forecast_final['total_units'].sum()\n",
        "total_revenue_2023 = optimised_forecast_final['total_revenue'].sum()\n",
        "total_profit_2023 = optimised_forecast_final['expected_profit'].sum()\n",
        "\n",
        "# Calculate 2022 totals.\n",
        "total_sales_2022 = sold_units_2022['sold_units'].sum()\n",
        "total_revenue_2022 = sold_units_2022['total_revenue_2022'].sum()\n",
        "total_profit_2022 = sold_units_2022['expected_profit_2022'].sum()\n",
        "\n",
        "# Calculate 2021 totals.\n",
        "total_sales_2021 = sold_units_2021['sold_units'].sum()\n",
        "total_revenue_2021 = sold_units_2021['total_revenue_2021'].sum()\n",
        "total_profit_2021 = sold_units_2021['expected_profit_2021'].sum()\n",
        "\n",
        "# Calculate margin of increase from 2022 to 2023.\n",
        "revenue_increase_2023_2022 = total_revenue_2023 - total_revenue_2022\n",
        "revenue_increase_cent_2023_2022 = (revenue_increase_2023_2022 / total_revenue_2022) * 100\n",
        "\n",
        "profit_increase_2023_2022 = total_profit_2023 - total_profit_2022\n",
        "profit_increase_cent_2023_2022 = (profit_increase_2023_2022 / total_profit_2022) * 100\n",
        "\n",
        "# Calculate margin of increase from 2021 to 2022.\n",
        "revenue_increase_2022_2021 = total_revenue_2022 - total_revenue_2021\n",
        "revenue_increase_cent_2022_2021 = (revenue_increase_2022_2021 / total_revenue_2021) * 100\n",
        "\n",
        "profit_increase_2022_2021 = total_profit_2022 - total_profit_2021\n",
        "profit_increase_cent_2022_2021 = (profit_increase_2022_2021 / total_profit_2021) * 100\n",
        "\n",
        "# View total units of 2021, 2022 & 2023\n",
        "print(f'Total sold units for 2021: {total_sales_2021:.2f}')\n",
        "print(f'Total sold units for 2022: {total_sales_2022:.2f}')\n",
        "print(f'Total forecasted units for 2023: {total_sales_2023:.2f}')\n",
        "print()\n",
        "\n",
        "# View revenue margin increases.\n",
        "print(f'Total revenue for 2021: {total_revenue_2021:.2f}')\n",
        "print(f'Total revenue for 2022: {total_revenue_2022:.2f}')\n",
        "print(f'Revenue increase 2022 on 2021: {revenue_increase_2022_2021:.2f}, {revenue_increase_cent_2022_2021:.2f}%\\n')\n",
        "\n",
        "print(f'Total revenue for 2022: {total_revenue_2022:.2f}')\n",
        "print(f'Total revenue for 2023: {total_revenue_2023:.2f}')\n",
        "print(f'Revenue increase 2023 on 2022: {revenue_increase_2023_2022:.2f}, {revenue_increase_cent_2023_2022:.2f}%\\n')\n",
        "\n",
        "# View profit margin increases.\n",
        "print(f'Total profit for 2021: {total_profit_2021:.2f}')\n",
        "print(f'Total profit for 2022: {total_profit_2022:.2f}')\n",
        "print(f'Revenue profit 2022 on 2021: {profit_increase_2022_2021:.2f}, {profit_increase_cent_2022_2021:.2f}%\\n')\n",
        "\n",
        "print(f'Total profit for 2022: {total_profit_2022:.2f}')\n",
        "print(f'Total profit for 2023: {total_profit_2023:.2f}')\n",
        "print(f'Profit increase 2023 on 2022: {profit_increase_2023_2022}, {profit_increase_cent_2023_2022:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ1_Xd7TJVrT"
      },
      "source": [
        "#### __Calculate 2023 Profit with 20% Premium Product Cap__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZqT3oQ9Jeax"
      },
      "outputs": [],
      "source": [
        "# Run optimiser with a 20% cap for premium product.\n",
        "optimised_forecast_20_cent = adjust_ratios_calc_revenue(optimised_forecast_df, max_unit2_ratio=0.20)\n",
        "\n",
        "# Sense Check DataFrame.\n",
        "optimised_forecast_20_cent.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvf33BUyJgMa"
      },
      "outputs": [],
      "source": [
        "# Calculate 2023 totals and 20% cap for premium products.\n",
        "total_sales_2023_20_cent = optimised_forecast_20_cent['total_units'].sum()\n",
        "total_revenue_2023_20_cent = optimised_forecast_20_cent['total_revenue'].sum()\n",
        "total_profit_2023_20_cent = optimised_forecast_20_cent['expected_profit'].sum()\n",
        "\n",
        "# Calculate margin of increase from 2022 to 2023.\n",
        "revenue_increase_2023_2022_20_cent = total_revenue_2023_20_cent - total_revenue_2022\n",
        "revenue_increase_cent_2023_2022_20_cent = (revenue_increase_2023_2022_20_cent / total_revenue_2022) * 100\n",
        "\n",
        "profit_increase_2023_2022_20_cent = total_profit_2023_20_cent - total_profit_2022\n",
        "profit_increase_cent_2023_2022_20_cent = (profit_increase_2023_2022_20_cent / total_profit_2022) * 100\n",
        "\n",
        "# View total units of 2022 & 2023\n",
        "print(f'Total sold units for 2022: {total_sales_2022:.2f}')\n",
        "print(f'Total forecasted units for 2023: {total_sales_2023_20_cent:.2f}')\n",
        "print()\n",
        "\n",
        "# View revenue margin increases.\n",
        "print(f'Total revenue for 2022: {total_revenue_2022:.2f}')\n",
        "print(f'Total revenue for 2023: {total_revenue_2023_20_cent:.2f}')\n",
        "print(f'Revenue increase 2023 on 2022: {revenue_increase_2023_2022_20_cent:.2f}, {revenue_increase_cent_2023_2022_20_cent:.2f}%\\n')\n",
        "\n",
        "# View profit margin increases.\n",
        "print(f'Total profit for 2022: {total_profit_2022:.2f}')\n",
        "print(f'Total profit for 2023: {total_profit_2023_20_cent:.2f}')\n",
        "print(f'Profit increase 2023 on 2022: {profit_increase_2023_2022_20_cent}, {profit_increase_cent_2023_2022_20_cent:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs339-Kit3Hm"
      },
      "source": [
        "#### __Prepare for Dashboard Visualisation__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-Ndbmcft-vX"
      },
      "outputs": [],
      "source": [
        "# Sense check unique event days to crosscheck with visualisation calculations.\n",
        "# Filter rows where event day is True\n",
        "event_days = optimised_forecast_final[optimised_forecast_final['is_event_day']]\n",
        "\n",
        "# Create YearMonth period.\n",
        "event_days['YearMonth'] = event_days.index.to_period('M')\n",
        "\n",
        "# Count unique event days per month.\n",
        "unique_event_days_per_month = event_days.groupby('YearMonth').apply(lambda x: pd.Series(x.index.date).nunique())\n",
        "\n",
        "# Reset index for nicer output.\n",
        "unique_event_days_per_month = unique_event_days_per_month.reset_index(name='UniqueEventDays')\n",
        "\n",
        "print(unique_event_days_per_month)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZtv6pTauAz8"
      },
      "outputs": [],
      "source": [
        "# Download final DataFrames for dashboarding and sense check function outputs.\n",
        "optimised_forecast_final.to_csv('/content/drive/MyDrive/Employer Project/Datasets/Production Data/filtered_sales_temp_hols_fmatches/optimised_forecast_final.csv', index=True)\n",
        "\n",
        "sold_units_2022_monthly.to_csv('/content/drive/MyDrive/Employer Project/Datasets/Production Data/filtered_sales_temp_hols_fmatches/sold_units_2022_monthly.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt-_DVFy3jdL"
      },
      "source": [
        "Revenue optimisation visualisations are located in Tableau file"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}